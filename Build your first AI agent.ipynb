{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fee2e6e0-b096-412c-b2d7-583183dbe455",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Quickstart: Build, test, and deploy an agent using Mosaic AI Agent Framework\n",
    "This quickstart notebook demonstrates how to build, test, and deploy a generative AI agent ([AWS](https://docs.databricks.com/aws/en/generative-ai/guide/introduction-generative-ai-apps#what-are-gen-ai-apps) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/guide/introduction-generative-ai-apps#what-are-gen-ai-apps) | [GCP](https://docs.databricks.com/gcp/en/generative-ai/guide/introduction-generative-ai-apps)) using Mosaic AI Agent Framework ([AWS](https://docs.databricks.com/aws/en/generative-ai/agent-framework/build-genai-apps#-mosaic-ai-agent-framework) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/build-genai-apps#-mosaic-ai-agent-framework) | [GCP](https://docs.databricks.com/gcp/en/generative-ai/agent-framework/build-genai-apps#-mosaic-ai-agent-framework)) on Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaa5ca05-0445-456e-a3ba-7512b48e0b4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define and test an agent\n",
    "This section defines and tests a simple agent with the following attributes:\n",
    "\n",
    "- The agent uses an LLM served on Databricks Foundation Model API ([AWS](https://docs.databricks.com/aws/en/machine-learning/foundation-model-apis) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/foundation-model-apis/) | [GCP](https://docs.databricks.com/gcp/en/machine-learning/foundation-model-apis))\n",
    "- The agent has access to a single tool, the built-in Python code interpreter tool on Databricks Unity Catalog. It can use this tool to run LLM-generated code in order to respond to user questions. ([AWS](https://docs.databricks.com/aws/en/generative-ai/agent-framework/code-interpreter-tools#built-in-python-executor-tool) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/code-interpreter-tools) | [GCP](https://docs.databricks.com/gcp/en/generative-ai/agent-framework/code-interpreter-tools))\n",
    "\n",
    "We will use `databricks_openai` SDK ([AWS](https://docs.databricks.com/aws/en/generative-ai/agent-framework/author-agent#requirements) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/author-agent#requirements) | [GCP](https://docs.databricks.com/gcp/en/generative-ai/agent-framework/author-agent#requirements)) to query the LLM endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "001e4416-a564-4620-b0ee-e577d0c1def8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow databricks-openai databricks-agents\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8e1af3e-1606-4f0e-bfdf-47a9e30546c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The snippet below tries to pick the first LLM API available in your Databricks workspace\n",
    "# from a set of candidates. You can override and simplify it\n",
    "# to just specify LLM_ENDPOINT_NAME.\n",
    "LLM_ENDPOINT_NAME = None\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "def is_endpoint_available(endpoint_name):\n",
    "  try:\n",
    "    client = WorkspaceClient().serving_endpoints.get_open_ai_client()\n",
    "    client.chat.completions.create(model=endpoint_name, messages=[{\"role\": \"user\", \"content\": \"What is AI?\"}])\n",
    "    return True\n",
    "  except Exception:\n",
    "    return False\n",
    "  \n",
    "client = WorkspaceClient()\n",
    "for candidate_endpoint_name in [\"databricks-claude-3-7-sonnet\", \"databricks-meta-llama-3-3-70b-instruct\"]:\n",
    "    if is_endpoint_available(candidate_endpoint_name):\n",
    "      LLM_ENDPOINT_NAME = candidate_endpoint_name\n",
    "assert LLM_ENDPOINT_NAME is not None, \"Please specify LLM_ENDPOINT_NAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fa8540e-a4fd-4221-93dd-dc390a045695",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Refresh UC function metadata (avoids \"function not found\" errors)"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Refresh Unity Catalog metadata for system.ai functions\n",
    "# This ensures the python_exec tool is discoverable in your workspace\n",
    "w = WorkspaceClient()\n",
    "list(w.schemas.list(\"system\"))\n",
    "w.schemas.get(\"system.ai\")\n",
    "_ = list(w.functions.list(catalog_name=\"system\", schema_name=\"ai\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7061396-f562-4878-96af-6e445ead8701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import mlflow\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_openai import UCFunctionToolkit, DatabricksFunctionClient\n",
    "# Import MLflow utilities for converting from chat completions to responses API format\n",
    "from mlflow.types.responses import output_to_responses_items_stream, create_function_call_output_item\n",
    "\n",
    "# Automatically log traces from LLM calls for ease of debugging\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Get an OpenAI client configured to talk to Databricks model serving endpoints\n",
    "# We'll use this to query an LLM in our agent\n",
    "openai_client = WorkspaceClient().serving_endpoints.get_open_ai_client()\n",
    "\n",
    "# Load Databricks built-in tools (a stateless Python code interpreter tool)\n",
    "client = DatabricksFunctionClient()\n",
    "builtin_tools = UCFunctionToolkit(\n",
    "    function_names=[\"system.ai.python_exec\"], client=client\n",
    ").tools\n",
    "for tool in builtin_tools:\n",
    "    del tool[\"function\"][\"strict\"]\n",
    "\n",
    "\n",
    "def call_tool(tool_name, parameters):\n",
    "    if tool_name == \"system__ai__python_exec\":\n",
    "        return DatabricksFunctionClient().execute_function(\n",
    "            \"system.ai.python_exec\", parameters=parameters\n",
    "        ).value\n",
    "    raise ValueError(f\"Unknown tool: {tool_name}\")\n",
    "\n",
    "def call_llm(prompt):\n",
    "    for chunk in openai_client.chat.completions.create(\n",
    "        model=LLM_ENDPOINT_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        tools=builtin_tools,\n",
    "        stream=True\n",
    "    ):\n",
    "        yield chunk.to_dict()\n",
    "\n",
    "def run_agent(prompt):\n",
    "    \"\"\"\n",
    "    Send a user prompt to the LLM, and yield LLM + tool call responses\n",
    "    The LLM is allowed to call the code interpreter tool if needed, to respond to the user\n",
    "    \"\"\"\n",
    "    # Convert output into Responses API-compatible events\n",
    "    for chunk in output_to_responses_items_stream(call_llm(prompt)):\n",
    "        yield chunk.model_dump(exclude_none=True)\n",
    "    # If the model executed a tool, call it and yield the tool call output in Responses API format\n",
    "    if chunk.item.get('type') == 'function_call':\n",
    "        tool_name = chunk.item[\"name\"]\n",
    "        tool_args = json.loads(chunk.item[\"arguments\"])\n",
    "        tool_result = call_tool(tool_name, tool_args)\n",
    "        yield {\"type\": \"response.output_item.done\", \"item\": create_function_call_output_item(call_id=chunk.item[\"call_id\"], output=tool_result)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "914c7603-d6e1-4e13-9a5e-0c2aac2a247c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for output_chunk in run_agent(\"What is the square root of 429?\"):\n",
    "    print(output_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c467353b-805a-4532-a753-dd55ec897ee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare agent code for logging\n",
    "\n",
    "Wrap your agent definition in MLflow’s [ResponsesAgent interface](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.pyfunc.html#mlflow.pyfunc.ResponsesAgent) to prepare your code for logging.\n",
    "\n",
    "By using MLflow’s standard agent authoring interface, you get built-in UIs for chatting with your agent and sharing it with others after deployment. ([AWS](https://docs.databricks.com/aws/en/generative-ai/agent-framework/author-agent#-use-chatagent-to-author-agents) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/author-agent) | [GCP](https://docs.databricks.com/gcp/en/generative-ai/agent-framework/author-agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "091e3c78-d09d-4de6-bd28-66636b34ae8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import mlflow\n",
    "from typing import Any, Optional, Generator\n",
    "\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import ResponsesAgentRequest, ResponsesAgentResponse, ResponsesAgentStreamEvent, output_to_responses_items_stream\n",
    "\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "class QuickstartAgent(ResponsesAgent):\n",
    "    def predict_stream(self, request: ResponsesAgentRequest): \n",
    "        # Extract the user's prompt from the request\n",
    "        prompt = request.input[-1].content\n",
    "        # Stream response items from our agent\n",
    "        for chunk in run_agent(prompt):\n",
    "            yield ResponsesAgentStreamEvent(**chunk)\n",
    "\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242dd426-982b-43d6-97b9-5e3c96ebc594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.types.responses import ResponsesAgentRequest\n",
    "\n",
    "AGENT = QuickstartAgent()\n",
    "\n",
    "# Create a proper ResponsesAgentRequest with input field\n",
    "request = ResponsesAgentRequest(\n",
    "    input=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"What's the square root of 429?\"\n",
    "            }\n",
    "    ]\n",
    ")\n",
    "\n",
    "for event in AGENT.predict_stream(request):\n",
    "    print(event)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84ae71be-a656-4453-bf84-69e7d876183a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log the agent\n",
    "\n",
    "Log the agent and register it to Unity Catalog as a model ([AWS](https://docs.databricks.com/aws/en/machine-learning/manage-model-lifecycle/) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/manage-model-lifecycle/) | [GCP](https://docs.databricks.com/gcp/en/machine-learning/manage-model-lifecycle/)). This step packages the agent code and its dependencies into a single artifact to deploy it to a serving endpoint.\n",
    "\n",
    "The following code cells do the following:\n",
    "\n",
    "1. Copy the agent code from above and combine it into a single cell.\n",
    "1. Add the `%%writefile` cell magic command at the top of the cell to save the agent code to a file called `quickstart_agent.py`.\n",
    "1. Add a [mlflow.models.set_model()](https://mlflow.org/docs/latest/model#models-from-code) call to the bottom of the cell. This tells MLflow which Python agent object to use for making predictions when your agent is deployed.\n",
    "1. Log the agent code in the `quickstart_agent.py` file using MLflow APIs ([AWS](https://docs.databricks.com/aws/en/generative-ai/agent-framework/log-agent) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/log-agent) | [GCP](https://docs.databricks.com/gcp/en/generative-ai/agent-framework/log-agent))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc036ad6-debc-4bb2-92df-575326ce2fa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile quickstart_agent.py\n",
    "\n",
    "import json\n",
    "import uuid\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_openai import UCFunctionToolkit, DatabricksFunctionClient\n",
    "from typing import Any, Optional, Generator\n",
    "\n",
    "import mlflow\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import ResponsesAgentRequest, ResponsesAgentStreamEvent, ResponsesAgentResponse, output_to_responses_items_stream, create_function_call_output_item\n",
    "\n",
    "\n",
    "# Get an OpenAI client configured to talk to Databricks model serving endpoints\n",
    "# We'll use this to query an LLM in our agent\n",
    "openai_client = WorkspaceClient().serving_endpoints.get_open_ai_client()\n",
    "\n",
    "# The snippet below tries to pick the first LLM API available in your Databricks workspace\n",
    "# from a set of candidates. You can override and simplify it\n",
    "# to just specify LLM_ENDPOINT_NAME.\n",
    "LLM_ENDPOINT_NAME = None\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "def is_endpoint_available(endpoint_name):\n",
    "  try:\n",
    "    client = WorkspaceClient().serving_endpoints.get_open_ai_client()\n",
    "    client.chat.completions.create(model=endpoint_name, messages=[{\"role\": \"user\", \"content\": \"What is AI?\"}])\n",
    "    return True\n",
    "  except Exception:\n",
    "    return False\n",
    "  \n",
    "client = WorkspaceClient()\n",
    "for candidate_endpoint_name in [\"databricks-claude-3-7-sonnet\", \"databricks-meta-llama-3-3-70b-instruct\"]:\n",
    "    if is_endpoint_available(candidate_endpoint_name):\n",
    "      LLM_ENDPOINT_NAME = candidate_endpoint_name\n",
    "assert LLM_ENDPOINT_NAME is not None, \"Please specify LLM_ENDPOINT_NAME\"\n",
    "\n",
    "# Automatically log traces from LLM calls for ease of debugging\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Get an OpenAI client configured to talk to Databricks model serving endpoints\n",
    "# We'll use this to query an LLM in our agent\n",
    "openai_client = WorkspaceClient().serving_endpoints.get_open_ai_client()\n",
    "\n",
    "# Load Databricks built-in tools (a stateless Python code interpreter tool)\n",
    "client = DatabricksFunctionClient()\n",
    "builtin_tools = UCFunctionToolkit(\n",
    "    function_names=[\"system.ai.python_exec\"], client=client\n",
    ").tools\n",
    "for tool in builtin_tools:\n",
    "    del tool[\"function\"][\"strict\"]\n",
    "\n",
    "\n",
    "def call_tool(tool_name, parameters):\n",
    "    if tool_name == \"system__ai__python_exec\":\n",
    "        return DatabricksFunctionClient().execute_function(\n",
    "            \"system.ai.python_exec\", parameters=parameters\n",
    "        ).value\n",
    "    raise ValueError(f\"Unknown tool: {tool_name}\")\n",
    "\n",
    "def call_llm(prompt):\n",
    "    for chunk in openai_client.chat.completions.create(\n",
    "        model=LLM_ENDPOINT_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        tools=builtin_tools,\n",
    "        stream=True\n",
    "    ):\n",
    "        yield chunk.to_dict()\n",
    "\n",
    "\n",
    "def run_agent(prompt):\n",
    "    \"\"\"\n",
    "    Send a user prompt to the LLM, and yield LLM + tool call responses\n",
    "    The LLM is allowed to call the code interpreter tool if needed, to respond to the user\n",
    "    \"\"\"\n",
    "    # Convert output into Responses API-compatible events\n",
    "    for chunk in output_to_responses_items_stream(call_llm(prompt)):\n",
    "        yield chunk.model_dump(exclude_none=True)\n",
    "    # If the model executed a tool, call it and yield the tool call output in Responses API format\n",
    "    if chunk.item.get('type') == 'function_call':\n",
    "        tool_name = chunk.item[\"name\"]\n",
    "        tool_args = json.loads(chunk.item[\"arguments\"])\n",
    "        tool_result = call_tool(tool_name, tool_args)\n",
    "        yield {\"type\": \"response.output_item.done\", \"item\": create_function_call_output_item(call_id=chunk.item[\"call_id\"], output=tool_result)}\n",
    "\n",
    "\n",
    "class QuickstartAgent(ResponsesAgent):\n",
    "    def predict_stream(self, request: ResponsesAgentRequest): \n",
    "        # Extract the user's prompt from the request\n",
    "        prompt = request.input[-1].content\n",
    "        # Stream response items from our agent\n",
    "        for chunk in run_agent(prompt):\n",
    "            yield ResponsesAgentStreamEvent(**chunk)\n",
    "\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs)\n",
    "\n",
    "AGENT = QuickstartAgent()\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73971aad-3e10-494d-bfaf-8a1a6c457a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a5d6402-ee23-4af3-9332-0555ebeb212c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test ResponsesAgent"
    }
   },
   "outputs": [],
   "source": [
    "# Test the ResponsesAgent implementation\n",
    "from quickstart_agent import QuickstartAgent, LLM_ENDPOINT_NAME\n",
    "from mlflow.types.responses import ResponsesAgentRequest\n",
    "\n",
    "print(f\"Using LLM endpoint: {LLM_ENDPOINT_NAME}\")\n",
    "\n",
    "# Create agent instance\n",
    "agent = QuickstartAgent()\n",
    "\n",
    "# Create test request - input should be a list of messages\n",
    "request = ResponsesAgentRequest(\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"What's the square root of 144?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Test the agent\n",
    "print(\"\\nTesting agent...\")\n",
    "response = agent.predict(request)\n",
    "print(f\"\\nAgent response:\")\n",
    "for i, output_item in enumerate(response.output):\n",
    "    print(f\"Item {i+1}: Type={output_item.type}\")\n",
    "    print(f\" {output_item}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5d18e11-4383-461b-af8a-d5cc41d3199d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\n",
    "from pkg_resources import get_distribution\n",
    "from quickstart_agent import LLM_ENDPOINT_NAME\n",
    "\n",
    "# Register the model to the workspace default catalog.\n",
    "# Specify a catalog (e.g. \"main\") and schema name (e.g. \"custom_schema\") if needed,\n",
    "# in order to register the agent to a different location\n",
    "default_catalog_name = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "catalog_name = default_catalog_name if default_catalog_name != \"hive_metastore\" else \"main\"\n",
    "schema_name = \"default\"\n",
    "registered_model_name = f\"{catalog_name}.{schema_name}.quickstart_agent\"\n",
    "\n",
    "# Specify Databricks product resources that the agent needs access to (our builtin python\n",
    "# code interpreter tool and LLM serving endpoint), so that Databricks can automatically\n",
    "# configure authentication for the agent to access these resources when it's deployed.\n",
    "resources = [\n",
    "    DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME),\n",
    "    DatabricksFunction(function_name=\"system.ai.python_exec\"),\n",
    "]\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"agent\",\n",
    "        python_model=\"quickstart_agent.py\",\n",
    "        extra_pip_requirements=[\n",
    "            f\"databricks-connect=={get_distribution('databricks-connect').version}\"\n",
    "        ],\n",
    "        resources=resources,\n",
    "        registered_model_name=registered_model_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90e76c4c-b53d-4d91-818e-b566e148ef8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy the agent\n",
    "\n",
    "Run the cell below to deploy the agent ([AWS](https://docs.databricks.com/aws/en/generative-ai/agent-framework/deploy-agent) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/deploy-agent) | [GCP](https://docs.databricks.com/gcp/en/generative-ai/agent-framework/deploy-agent)). Once the agent endpoint starts, you can chat with it via AI Playground ([AWS](https://docs.databricks.com/aws/en/large-language-models/ai-playground) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/large-language-models/ai-playground) | [GCP](https://docs.databricks.com/gcp/en/large-language-models/ai-playground)), or share it with stakeholders ([AWS](https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/review-app) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-evaluation/review-app) | [GCP](https://docs.databricks.com/gcp/en/generative-ai/agent-evaluation/review-app)) for initial feedback, before sharing it more broadly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba63fd49-8579-46cd-9a31-5b5f22bb7d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "deployment_info = agents.deploy(\n",
    "    model_name=registered_model_name,\n",
    "    model_version=logged_agent_info.registered_model_version,\n",
    "    scale_to_zero=True,\n",
    "    deploy_feedback_model=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Build your first AI agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
