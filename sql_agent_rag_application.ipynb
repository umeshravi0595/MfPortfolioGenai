{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0d1aae6-f2cd-497a-ba85-a6f4565d92eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install databricks-vectorsearch\n",
    "# %pip install --upgrade databricks-langchain langchain-community langchain databricks-sql-connector\n",
    "\n",
    "# %pip install -U mlflow\n",
    "\n",
    "# %pip install databricks-sql-connector pandas\n",
    "# dbutils.library.restartPython()\n",
    "import mlflow\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from databricks.vector_search.client import VectorSearchClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98f6d83b-3e71-410a-9101-fe377f8132a8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "token"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "# w.secrets.create_scope(\n",
    "#     scope=\"pizza-secrets\"\n",
    "# )\n",
    "# Now you can add your secret as before\n",
    "w.secrets.put_secret(\n",
    "    scope=\"pizza-secrets\",\n",
    "    key=\"DATABRICKS_TOKEN\",\n",
    "    string_value=dbutils.notebook.entry_point\n",
    "        .getDbutils()\n",
    "        .notebook()\n",
    "        .getContext()\n",
    "        .apiToken()\n",
    "        .get()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128f24c8-786b-4573-958f-43baee2ff173",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "databricks host"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "\n",
    "os.environ[\"DATABRICKS_HOST\"] = ctx.apiUrl().get()\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = ctx.apiToken().get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ed3d62-10b8-4293-93bf-069c64ee9355",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Metric & Entity Registry"
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ENTITY DEFINITIONS\n",
    "# =========================\n",
    "import yaml\n",
    "REGISTRY_YAML = \"\"\"entities:\n",
    "  pizza:\n",
    "    grain: pizza_name_id\n",
    "    tables:\n",
    "      monthly: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "      daily: retail_analytics.pizza.gold_pizza_metrics_daily\n",
    "    time_dimensions: [day, month, year]\n",
    "\n",
    "  ingredient:\n",
    "    grain: ingredient\n",
    "    tables:\n",
    "      usage: retail_analytics.pizza.gold_ingredient_usage\n",
    "    time_dimensions: [month, year]\n",
    "\n",
    "# =========================\n",
    "# BASE METRICS\n",
    "# =========================\n",
    "metrics:\n",
    "  total_pizzas_sold:\n",
    "    sql: SUM(total_pizzas_sold)\n",
    "    table: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "    entity: pizza\n",
    "\n",
    "  total_revenue:\n",
    "    sql: SUM(total_revenue)\n",
    "    table: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "    entity: pizza\n",
    "\n",
    "  daily_revenue:\n",
    "    sql: SUM(revenue_perday)\n",
    "    table: retail_analytics.pizza.gold_pizza_metrics_daily\n",
    "    entity: pizza\n",
    "\n",
    "  ingredient_usage_grams:\n",
    "    sql: SUM(total_ingredient_grams)\n",
    "    table: retail_analytics.pizza.gold_ingredient_usage\n",
    "    entity: ingredient\n",
    "\n",
    "  ingredient_items_qty_grams:\n",
    "    sql: SUM(items_qty_in_grams)\n",
    "    table: retail_analytics.pizza.gold_ingredient_usage\n",
    "    entity: ingredient\n",
    "\n",
    "# =========================\n",
    "# DERIVED METRICS\n",
    "# =========================\n",
    "derived_metrics:\n",
    "  average_daily_revenue:\n",
    "    sql: SUM(revenue_perday) / COUNT(DISTINCT day)\n",
    "    table: retail_analytics.pizza.gold_pizza_metrics_daily\n",
    "    entity: pizza\n",
    "\n",
    "  revenue_per_pizza:\n",
    "    sql: SUM(total_revenue) / SUM(total_pizzas_sold)\n",
    "    table: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "    entity: pizza\n",
    "\n",
    "  revenue_per_ingredient_gram:\n",
    "    sql: SUM(total_revenue) / SUM(total_ingredient_grams)\n",
    "    tables:\n",
    "      revenue: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "      ingredient: retail_analytics.pizza.gold_ingredient_usage\n",
    "    entity: ingredient\n",
    "\n",
    "# =========================\n",
    "# JOIN DEFINITIONS\n",
    "# =========================\n",
    "joins:\n",
    "  pizza_to_ingredient:\n",
    "    left_table: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "    right_table: retail_analytics.pizza.gold_ingredient_usage\n",
    "    on:\n",
    "      - pizza_name_id\n",
    "      - month\n",
    "      - year\n",
    "\n",
    "  pizza_daily_to_monthly:\n",
    "    left_table: retail_analytics.pizza.gold_pizza_metrics_daily\n",
    "    right_table: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "    on:\n",
    "      - pizza_name_id\n",
    "      - month\n",
    "      - year\n",
    "\n",
    "# =========================\n",
    "# TIME AGGREGATIONS\n",
    "# =========================\n",
    "time_grains:\n",
    "  daily:\n",
    "    columns: [day, month, year]\n",
    "\n",
    "  monthly:\n",
    "    columns: [month, year]\n",
    "\n",
    "  yearly:\n",
    "    columns: [year]\n",
    "\n",
    "# =========================\n",
    "# RANKING & WINDOW PATTERNS\n",
    "# =========================\n",
    "ranking_patterns:\n",
    "  highest:\n",
    "    order: DESC\n",
    "    limit: 1\n",
    "\n",
    "  lowest:\n",
    "    order: ASC\n",
    "    limit: 1\n",
    "\n",
    "  top_n:\n",
    "    order: DESC\n",
    "    limit: N\n",
    "\n",
    "  bottom_n:\n",
    "    order: ASC\n",
    "    limit: N\n",
    "\n",
    "window_patterns:\n",
    "  rank_within_time:\n",
    "    sql: RANK() OVER (PARTITION BY {time} ORDER BY {metric} DESC)\n",
    "\n",
    "  dense_rank_within_time:\n",
    "    sql: DENSE_RANK() OVER (PARTITION BY {time} ORDER BY {metric} DESC)\n",
    "\n",
    "# =========================\n",
    "# TREND PATTERNS\n",
    "# =========================\n",
    "trend_patterns:\n",
    "  month_over_month_change:\n",
    "    sql: \"{metric} - LAG({metric}) OVER (PARTITION BY {entity} ORDER BY year, month)\"\n",
    "\n",
    "  month_over_month_growth_pct:\n",
    "    sql: \"({metric} - LAG({metric}) OVER (PARTITION BY {entity} ORDER BY year, month))\n",
    "          / NULLIF(LAG({metric}) OVER (PARTITION BY {entity} ORDER BY year, month), 0)\"\n",
    "\n",
    "  consecutive_decline:\n",
    "    rule: \"negative change for N consecutive months\"\n",
    "\n",
    "# =========================\n",
    "# BUSINESS TERM DEFINITIONS\n",
    "# =========================\n",
    "business_terms:\n",
    "  peak_month:\n",
    "    definition: \"month with highest total_revenue\"\n",
    "\n",
    "  bottleneck:\n",
    "    definition: \"ingredient with highest total_ingredient_grams during peak_month\"\n",
    "\n",
    "  least_used:\n",
    "    definition: \"minimum total_ingredient_grams\"\n",
    "\n",
    "  top_selling:\n",
    "    definition: \"highest total_pizzas_sold\"\n",
    "\n",
    "  volatile:\n",
    "    definition: \"highest standard deviation of monthly revenue\"\n",
    "\n",
    "  declining_sales:\n",
    "    definition: \"negative month_over_month_change for 3 consecutive months\"\n",
    "\n",
    "# =========================\n",
    "# VALIDATION RULES\n",
    "# =========================\n",
    "validation:\n",
    "  forbid_columns:\n",
    "    - peak\n",
    "    - best\n",
    "    - highest\n",
    "  require_time_for_trends: true\n",
    "  require_join_for_derived_metrics: true\"\"\"\n",
    "REGISTRY = yaml.safe_load(REGISTRY_YAML)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "501c205b-1ff9-4091-9802-62f6c6ea3481",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "sql agent"
    }
   },
   "outputs": [],
   "source": [
    "class SQLAgent:\n",
    "    def __init__(self, registry):\n",
    "        self.registry = registry\n",
    "\n",
    "    def plan(self, intent):\n",
    "        return intent\n",
    "\n",
    "    def generate_sql(self, plan):\n",
    "        entity = plan[\"entity\"]\n",
    "        metric = plan[\"metric\"]\n",
    "        time_grain = plan.get(\"time_grain\")\n",
    "        window = plan.get(\"window\")\n",
    "        trend = plan.get(\"trend\")\n",
    "        ranking = plan.get(\"ranking\")\n",
    "        joins = plan.get(\"joins\", [])\n",
    "        filters = plan.get(\"filters\", [])\n",
    "\n",
    "        base_table = self.registry[\"entities\"][entity][\"table\"]\n",
    "        metric_sql = self.registry[\"metrics\"][metric][\"sql\"]\n",
    "\n",
    "        # ---- JOIN LOGIC ----\n",
    "        join_sql = []\n",
    "        for j in joins:\n",
    "            cfg = self.registry[\"entities\"][entity][\"joins\"][j]\n",
    "            join_sql.append(f\"JOIN {cfg['table']} USING ({cfg['on']})\")\n",
    "\n",
    "        # ---- SELECT / GROUP BY ----\n",
    "        select_cols = [entity]\n",
    "        group_cols = [entity]\n",
    "\n",
    "        if time_grain:\n",
    "            select_cols.append(time_grain)\n",
    "            group_cols.append(time_grain)\n",
    "\n",
    "        # ---- BASE CTE ----\n",
    "        sql = f\"\"\"\n",
    "WITH base AS (\n",
    "    SELECT\n",
    "        {', '.join(select_cols)},\n",
    "        {metric_sql} AS metric_value\n",
    "    FROM {base_table}\n",
    "    {' '.join(join_sql)}\n",
    "\"\"\"\n",
    "\n",
    "        if filters:\n",
    "            sql += f\"\"\"\n",
    "    WHERE {' AND '.join(filters)}\n",
    "\"\"\"\n",
    "\n",
    "        sql += f\"\"\"\n",
    "    GROUP BY {', '.join(group_cols)}\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "        # ---- WINDOW FUNCTION ----\n",
    "        if window:\n",
    "            window_sql = self.registry[\"window_patterns\"][window][\"sql\"].format(\n",
    "                time=time_grain,\n",
    "                metric=\"metric_value\"\n",
    "            )\n",
    "            sql += f\"\"\"\n",
    ", ranked AS (\n",
    "    SELECT *,\n",
    "           {window_sql} AS rank\n",
    "    FROM base\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "        # ---- TREND LOGIC ----\n",
    "        if trend:\n",
    "            trend_sql = self.registry[\"trend_patterns\"][trend][\"sql\"].format(\n",
    "                entity=entity,\n",
    "                time=time_grain,\n",
    "                metric=\"metric_value\"\n",
    "            )\n",
    "            sql += f\"\"\"\n",
    ", trended AS (\n",
    "    SELECT *,\n",
    "           {trend_sql} AS trend_value\n",
    "    FROM base\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "        final_cte = \"ranked\" if window else \"trended\" if trend else \"base\"\n",
    "\n",
    "        sql += f\"\"\"\n",
    "SELECT *\n",
    "FROM {final_cte}\n",
    "\"\"\"\n",
    "\n",
    "        if ranking:\n",
    "            r = self.registry[\"ranking\"][ranking]\n",
    "            sql += f\"\"\"\n",
    "ORDER BY metric_value {r['order']}\n",
    "LIMIT {r['limit']}\n",
    "\"\"\"\n",
    "\n",
    "        return sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b231a0d-34fb-496b-ab08-f2b39a65b227",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "llm intent resolver"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def resolve_intent(llm, question):\n",
    "    prompt = f\"\"\"\n",
    "    Output STRICT JSON only.\n",
    "\n",
    "    {{\n",
    "      \"entity\": \"pizza|ingredient\",\n",
    "      \"metric\": \"revenue|pizzas_sold|ingredient_usage_grams\",\n",
    "      \"time_grain\": \"month|quarter|year|null\",\n",
    "      \"window\": \"rank_by_time|null\",\n",
    "      \"trend\": \"mom_growth|null\",\n",
    "      \"ranking\": \"highest|lowest|null\",\n",
    "      \"joins\": [],\n",
    "      \"filters\": []\n",
    "    }}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    return json.loads(llm.invoke(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cde4699b-cca3-4f58-978a-2146a7879fc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def execute_with_repair(spark, sql, retries=2):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            return spark.sql(sql)\n",
    "        except Exception as e:\n",
    "            if i == retries - 1:\n",
    "                raise RuntimeError(f\"SQL failed:\\n{sql}\\n\\n{e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e7138da-017b-425a-ab78-2bfee7494d64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def interpret_result(llm, question, df):\n",
    "    data = df.limit(20).toPandas().to_dict(orient=\"records\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Explain the result using ONLY the data below.\n",
    "    Do not invent metrics.\n",
    "\n",
    "    Question: {question}\n",
    "    Data: {data}\n",
    "    \"\"\"\n",
    "    return llm.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "446c24c6-fa7a-4918-aad0-732c98c73ba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class PizzaAnalyticsAgent(mlflow.pyfunc.PythonModel):\n",
    "\n",
    "    def load_context(self, context):\n",
    "        self.registry = REGISTRY\n",
    "        self.sql_agent = SQLAgent(self.registry)\n",
    "        self.llm = context.artifacts[\"llm_client\"]\n",
    "        self.spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        question = model_input[\"question\"]\n",
    "\n",
    "        intent = resolve_intent(self.llm, question)\n",
    "        sql = self.sql_agent.generate_sql(intent)\n",
    "        df = execute_with_repair(self.spark, sql)\n",
    "        answer = interpret_result(self.llm, question, df)\n",
    "\n",
    "        return {\n",
    "            \"sql\": sql,\n",
    "            \"answer\": answer,\n",
    "            \"preview\": df.limit(10).toPandas()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec6df854-8e23-4947-a953-ab83ae1ec49f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "llm_client = OpenAI(\n",
    "    base_url=f\"{os.environ['DATABRICKS_HOST']}/serving-endpoints\",\n",
    "    api_key=os.environ[\"DATABRICKS_TOKEN\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c8403c1-3049-4d67-a236-e1691aea86da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aab43c32-4bea-4c76-bcb7-846a0a8209bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.pyfunc.log_model(\n",
    "    artifact_path=\"pizza_analytics_agent\",\n",
    "    python_model=PizzaAnalyticsAgent(),\n",
    "    artifacts={\n",
    "        \"llm_client\": llm_client   # your Groq / OpenAI / Databricks LLM\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "266912ee-ea1f-4e5c-a1d7-ec3f811489dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import yaml\n",
    "import json\n",
    "# from pyspark.sql import SparkSession\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from mlflow.models import ModelSignature\n",
    "from mlflow.types import Schema, ColSpec\n",
    "\n",
    "# -------------------------\n",
    "# 1️⃣ Registry YAML (embedded)\n",
    "# -------------------------\n",
    "REGISTRY_YAML = \"\"\"\n",
    "entities:\n",
    "  pizza:\n",
    "    grain: pizza_name_id\n",
    "    tables:\n",
    "      monthly: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "      daily: retail_analytics.pizza.gold_pizza_metrics_daily\n",
    "    time_dimensions: [day, month, year]\n",
    "\n",
    "  ingredient:\n",
    "    grain: ingredient\n",
    "    tables:\n",
    "      usage: retail_analytics.pizza.gold_ingredient_usage\n",
    "    time_dimensions: [month, year]\n",
    "\n",
    "# =========================\n",
    "# BASE METRICS\n",
    "# =========================\n",
    "metrics:\n",
    "  total_pizzas_sold:\n",
    "    sql: SUM(total_pizzas_sold)\n",
    "    table: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "    entity: pizza\n",
    "\n",
    "  total_revenue:\n",
    "    sql: SUM(total_revenue)\n",
    "    table: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "    entity: pizza\n",
    "\n",
    "  daily_revenue:\n",
    "    sql: SUM(revenue_perday)\n",
    "    table: retail_analytics.pizza.gold_pizza_metrics_daily\n",
    "    entity: pizza\n",
    "\n",
    "  ingredient_usage_grams:\n",
    "    sql: SUM(total_ingredient_grams)\n",
    "    table: retail_analytics.pizza.gold_ingredient_usage\n",
    "    entity: ingredient\n",
    "\n",
    "  ingredient_items_qty_grams:\n",
    "    sql: SUM(items_qty_in_grams)\n",
    "    table: retail_analytics.pizza.gold_ingredient_usage\n",
    "    entity: ingredient\n",
    "\n",
    "# =========================\n",
    "# DERIVED METRICS\n",
    "# =========================\n",
    "derived_metrics:\n",
    "  average_daily_revenue:\n",
    "    sql: SUM(revenue_perday) / COUNT(DISTINCT day)\n",
    "    table: retail_analytics.pizza.gold_pizza_metrics_daily\n",
    "    entity: pizza\n",
    "\n",
    "  revenue_per_pizza:\n",
    "    sql: SUM(total_revenue) / SUM(total_pizzas_sold)\n",
    "    table: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "    entity: pizza\n",
    "\n",
    "  revenue_per_ingredient_gram:\n",
    "    sql: SUM(total_revenue) / SUM(total_ingredient_grams)\n",
    "    tables:\n",
    "      revenue: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "      ingredient: retail_analytics.pizza.gold_ingredient_usage\n",
    "    entity: ingredient\n",
    "\n",
    "# =========================\n",
    "# JOIN DEFINITIONS\n",
    "# =========================\n",
    "joins:\n",
    "  pizza_to_ingredient:\n",
    "    left_table: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "    right_table: retail_analytics.pizza.gold_ingredient_usage\n",
    "    on:\n",
    "      - pizza_name_id\n",
    "      - month\n",
    "      - year\n",
    "\n",
    "  pizza_daily_to_monthly:\n",
    "    left_table: retail_analytics.pizza.gold_pizza_metrics_daily\n",
    "    right_table: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "    on:\n",
    "      - pizza_name_id\n",
    "      - month\n",
    "      - year\n",
    "\n",
    "# =========================\n",
    "# TIME AGGREGATIONS\n",
    "# =========================\n",
    "time_grains:\n",
    "  daily:\n",
    "    columns: [day, month, year]\n",
    "\n",
    "  monthly:\n",
    "    columns: [month, year]\n",
    "\n",
    "  yearly:\n",
    "    columns: [year]\n",
    "\n",
    "# =========================\n",
    "# RANKING & WINDOW PATTERNS\n",
    "# =========================\n",
    "ranking_patterns:\n",
    "  highest:\n",
    "    order: DESC\n",
    "    limit: 1\n",
    "\n",
    "  lowest:\n",
    "    order: ASC\n",
    "    limit: 1\n",
    "\n",
    "  top_n:\n",
    "    order: DESC\n",
    "    limit: N\n",
    "\n",
    "  bottom_n:\n",
    "    order: ASC\n",
    "    limit: N\n",
    "\n",
    "window_patterns:\n",
    "  rank_within_time:\n",
    "    sql: RANK() OVER (PARTITION BY {time} ORDER BY {metric} DESC)\n",
    "\n",
    "  dense_rank_within_time:\n",
    "    sql: DENSE_RANK() OVER (PARTITION BY {time} ORDER BY {metric} DESC)\n",
    "\n",
    "# =========================\n",
    "# TREND PATTERNS\n",
    "# =========================\n",
    "trend_patterns:\n",
    "  month_over_month_change:\n",
    "    sql: \"{metric} - LAG({metric}) OVER (PARTITION BY {entity} ORDER BY year, month)\"\n",
    "\n",
    "  month_over_month_growth_pct:\n",
    "    sql: \"({metric} - LAG({metric}) OVER (PARTITION BY {entity} ORDER BY year, month))\n",
    "          / NULLIF(LAG({metric}) OVER (PARTITION BY {entity} ORDER BY year, month), 0)\"\n",
    "\n",
    "  consecutive_decline:\n",
    "    rule: \"negative change for N consecutive months\"\n",
    "\n",
    "# =========================\n",
    "# BUSINESS TERM DEFINITIONS\n",
    "# =========================\n",
    "business_terms:\n",
    "  peak_month:\n",
    "    definition: \"month with highest total_revenue\"\n",
    "\n",
    "  bottleneck:\n",
    "    definition: \"ingredient with highest total_ingredient_grams during peak_month\"\n",
    "\n",
    "  least_used:\n",
    "    definition: \"minimum total_ingredient_grams\"\n",
    "\n",
    "  top_selling:\n",
    "    definition: \"highest total_pizzas_sold\"\n",
    "\n",
    "  volatile:\n",
    "    definition: \"highest standard deviation of monthly revenue\"\n",
    "\n",
    "  declining_sales:\n",
    "    definition: \"negative month_over_month_change for 3 consecutive months\"\n",
    "\n",
    "# =========================\n",
    "# VALIDATION RULES\n",
    "# =========================\n",
    "validation:\n",
    "  forbid_columns:\n",
    "    - peak\n",
    "    - best\n",
    "    - highest\n",
    "  require_time_for_trends: true\n",
    "  require_join_for_derived_metrics: true\"\"\"\n",
    "REGISTRY = yaml.safe_load(REGISTRY_YAML)\n",
    "\n",
    "# -------------------------\n",
    "# 2️⃣ SQLAgent\n",
    "# -------------------------\n",
    "class SQLAgent:\n",
    "    def __init__(self, registry):\n",
    "        self.registry = registry\n",
    "\n",
    "    def plan(self, intent):\n",
    "        return intent\n",
    "\n",
    "    def generate_sql(self, intent):\n",
    "        entity = intent[\"entity\"]\n",
    "        metric = intent[\"metric\"]\n",
    "        time_grain = intent.get(\"time_grain\")  # day / month / year / None\n",
    "        window = intent.get(\"window\")\n",
    "        trend = intent.get(\"trend\")\n",
    "        ranking = intent.get(\"ranking\")\n",
    "        joins = intent.get(\"joins\", [])\n",
    "        filters = intent.get(\"filters\", [])\n",
    "\n",
    "        # -----------------------------\n",
    "        # 1️⃣ Determine base table\n",
    "        # -----------------------------\n",
    "        # Use daily table for day-level queries, monthly for month/year\n",
    "        if time_grain == \"day\":\n",
    "            base_table = self.registry[\"entities\"][entity][\"tables\"].get(\"daily\")\n",
    "        else:\n",
    "            base_table = self.registry[\"entities\"][entity][\"tables\"].get(\"monthly\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2️⃣ Determine metric column\n",
    "        # -----------------------------\n",
    "        if time_grain == \"day\" and metric in [\"total_revenue\", \"daily_revenue\"]:\n",
    "            metric_sql = \"SUM(revenue_perday)\"\n",
    "        else:\n",
    "            metric_sql = self.registry[\"metrics\"][metric][\"sql\"]\n",
    "\n",
    "        # -----------------------------\n",
    "        # 3️⃣ Build SELECT & GROUP BY\n",
    "        # -----------------------------\n",
    "        select_cols = [entity]\n",
    "        group_cols = [entity]\n",
    "\n",
    "        # Add time_grain columns\n",
    "        if time_grain == \"day\":\n",
    "            select_cols += [\"day\", \"month\", \"year\"]\n",
    "            group_cols += [\"day\", \"month\", \"year\"]\n",
    "        elif time_grain == \"month\":\n",
    "            select_cols += [\"month\", \"year\"]\n",
    "            group_cols += [\"month\", \"year\"]\n",
    "        elif time_grain == \"year\":\n",
    "            select_cols += [\"year\"]\n",
    "            group_cols += [\"year\"]\n",
    "\n",
    "        # -----------------------------\n",
    "        # 4️⃣ Build JOINs\n",
    "        # -----------------------------\n",
    "        join_sql = []\n",
    "        for j in joins:\n",
    "            cfg = self.registry[\"joins\"][j]\n",
    "            join_sql.append(f\"JOIN {cfg['right_table']} USING ({', '.join(cfg['on'])})\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # 5️⃣ Build WHERE filters\n",
    "        # -----------------------------\n",
    "        filter_clauses = []\n",
    "        for f in filters:\n",
    "            field = f.get(\"field\")\n",
    "            op = f.get(\"operator\", \"=\")\n",
    "            value = f.get(\"value\")\n",
    "            if isinstance(value, str):\n",
    "                value = f\"'{value}'\"\n",
    "            filter_clauses.append(f\"{field} {op} {value}\")\n",
    "\n",
    "        where_sql = f\"WHERE {' AND '.join(filter_clauses)}\" if filter_clauses else \"\"\n",
    "\n",
    "        # -----------------------------\n",
    "        # 6️⃣ Base CTE\n",
    "        # -----------------------------\n",
    "        sql = f\"\"\"\n",
    "WITH base AS (\n",
    "    SELECT\n",
    "        {', '.join(select_cols)},\n",
    "        {metric_sql} AS metric_value\n",
    "    FROM {base_table}\n",
    "    {' '.join(join_sql)}\n",
    "    {where_sql}\n",
    "    GROUP BY {', '.join(group_cols)}\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "        # -----------------------------\n",
    "        # 7️⃣ Apply window function\n",
    "        # -----------------------------\n",
    "        if window:\n",
    "            window_sql = self.registry[\"window_patterns\"][window][\"sql\"].format(\n",
    "                time=\"_\".join(group_cols),  # partition by all grouping columns\n",
    "                metric=\"metric_value\"\n",
    "            )\n",
    "            sql += f\"\"\"\n",
    ", ranked AS (\n",
    "    SELECT *,\n",
    "           {window_sql} AS rank\n",
    "    FROM base\n",
    ")\n",
    "\"\"\"\n",
    "            final_cte = \"ranked\"\n",
    "        else:\n",
    "            final_cte = \"base\"\n",
    "\n",
    "        # -----------------------------\n",
    "        # 8️⃣ Apply trend function\n",
    "        # -----------------------------\n",
    "        if trend:\n",
    "            trend_sql = self.registry[\"trend_patterns\"][trend][\"sql\"].format(\n",
    "                entity=entity,\n",
    "                metric=\"metric_value\",\n",
    "                time=\"_\".join(group_cols)\n",
    "            )\n",
    "            sql += f\"\"\"\n",
    ", trended AS (\n",
    "    SELECT *,\n",
    "           {trend_sql} AS trend_value\n",
    "    FROM base\n",
    ")\n",
    "\"\"\"\n",
    "            final_cte = \"trended\"\n",
    "\n",
    "        # -----------------------------\n",
    "        # 9️⃣ Final SELECT + ranking\n",
    "        # -----------------------------\n",
    "        sql += f\"SELECT * FROM {final_cte}\\n\"\n",
    "\n",
    "        if ranking:\n",
    "            r = self.registry[\"ranking_patterns\"][ranking]\n",
    "            limit_val = r['limit'] if isinstance(r['limit'], int) else 'N'\n",
    "            sql += f\"ORDER BY metric_value {r['order']} LIMIT {limit_val}\\n\"\n",
    "\n",
    "        return sql\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3️⃣ LLM client helper (works notebook + serving)\n",
    "# -------------------------\n",
    "def get_llm():\n",
    "    return OpenAI(\n",
    "        base_url=f\"{os.environ['DATABRICKS_HOST']}/serving-endpoints\",\n",
    "        api_key=os.environ[\"DATABRICKS_TOKEN\"],\n",
    "    )\n",
    "import json\n",
    "import ast\n",
    "\n",
    "import json\n",
    "import re\n",
    "import json\n",
    "import re\n",
    "\n",
    "def extract_json_from_llm(raw):\n",
    "    \"\"\"\n",
    "    Extract last JSON object from LLM output.\n",
    "    Handles:\n",
    "    - list outputs from Databricks LLM\n",
    "    - extra reasoning text before JSON\n",
    "    \"\"\"\n",
    "    # If list output, join all texts\n",
    "    if isinstance(raw, list):\n",
    "        raw = \"\".join([item.get(\"text\", str(item)) if isinstance(item, dict) else str(item) for item in raw])\n",
    "\n",
    "    # Regex to find JSON object\n",
    "    matches = re.findall(r'(\\{.*\\})', raw, flags=re.DOTALL)\n",
    "    if not matches:\n",
    "        raise ValueError(f\"No JSON object found in LLM output:\\n{raw}\")\n",
    "\n",
    "    # Use the last JSON object\n",
    "    json_str = matches[-1]\n",
    "\n",
    "    # Replace single quotes with double quotes (naive fix)\n",
    "    json_str = json_str.replace(\"'\", '\"')\n",
    "\n",
    "    # Remove trailing commas\n",
    "    json_str = re.sub(r\",\\s*}\", \"}\", json_str)\n",
    "    json_str = re.sub(r\",\\s*]\", \"]\", json_str)\n",
    "\n",
    "    try:\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Failed to parse extracted JSON:\\n{json_str}\\nError: {e}\")\n",
    "\n",
    "\n",
    "def extract_intent_json(raw):\n",
    "    \"\"\"\n",
    "    Extract valid intent JSON from raw LLM output.\n",
    "\n",
    "    Handles:\n",
    "    - list outputs from Databricks LLM\n",
    "    - extra reasoning text before JSON\n",
    "    - JSON array or single JSON object\n",
    "    \"\"\"\n",
    "    # --- Step 1: Convert list outputs to string ---\n",
    "    if isinstance(raw, list):\n",
    "        texts = []\n",
    "        for item in raw:\n",
    "            if isinstance(item, dict) and \"text\" in item:\n",
    "                texts.append(item[\"text\"])\n",
    "            else:\n",
    "                texts.append(str(item))\n",
    "        raw = \"\".join(texts)\n",
    "        print(raw)\n",
    "\n",
    "    # --- Step 2: Extract all JSON objects or arrays using regex ---\n",
    "    matches = re.findall(r'(\\{.*?\\}|\\[.*?\\])', raw, flags=re.DOTALL)\n",
    "    if not matches:\n",
    "        raise ValueError(f\"No JSON found in LLM output:\\n{raw}\")\n",
    "\n",
    "    # --- Step 3: Pick last valid JSON containing 'entity' ---\n",
    "    for m in reversed(matches):\n",
    "        try:\n",
    "            candidate = json.loads(m)\n",
    "            # Accept dict or list of dicts with 'entity' key\n",
    "            if isinstance(candidate, dict) and \"entity\" in candidate:\n",
    "                return candidate\n",
    "            if isinstance(candidate, list) and all(isinstance(c, dict) and \"entity\" in c for c in candidate):\n",
    "                return candidate\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    raise ValueError(f\"No valid intent JSON found in LLM output:\\n{raw}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def resolve_intent(llm, question: str) -> dict:\n",
    "    DEFAULT_INTENT = {\n",
    "        \"entity\": \"pizza\",\n",
    "        \"metric\": \"total_revenue\",\n",
    "        \"time_grain\": None,\n",
    "        \"window\": None,\n",
    "        \"trend\": None,\n",
    "        \"ranking\": None,\n",
    "        \"joins\": [],\n",
    "        \"filters\": []\n",
    "    }\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Return STRICT JSON only.\n",
    "\n",
    "Valid values:\n",
    "- entity: pizza | ingredient\n",
    "- metric: total_revenue | total_pizzas_sold | ingredient_usage_grams\n",
    "- time_grain: day | month | year | null\n",
    "- window: rank_within_time | null\n",
    "- trend: month_over_month_growth_pct | null\n",
    "- ranking: highest | lowest | null\n",
    "- joins: []\n",
    "- filters: []\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.chat.completions.create(\n",
    "        model=\"databricks-gpt-oss-120b\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You output ONLY valid JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    raw = response.choices[0].message.content\n",
    "    parsed = extract_intent_json(raw)\n",
    "    # ✅ SAFE NORMALIZATION (THIS FIXES YOUR ERROR)\n",
    "    # if isinstance(raw, dict):\n",
    "    #     parsed = raw\n",
    "    # elif isinstance(raw, str):\n",
    "    #     parsed = json.loads(raw)\n",
    "    # else:\n",
    "    #     raise ValueError(f\"Unsupported LLM output type: {type(raw)}\")\n",
    "\n",
    "    if isinstance(parsed, list):\n",
    "        intents = []\n",
    "        for p in parsed:\n",
    "            intent = DEFAULT_INTENT.copy()\n",
    "            intent.update(p)\n",
    "            intents.append(intent)\n",
    "        return intents\n",
    "    else:\n",
    "        intent = DEFAULT_INTENT.copy()\n",
    "        intent.update(parsed)\n",
    "        return intent\n",
    "\n",
    "# -------------------------\n",
    "# 4️⃣ Result interpreter\n",
    "# -------------------------\n",
    "def interpret_result(llm, question, df):\n",
    "    data = df.limit(20).toPandas().to_dict(orient=\"records\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Explain the SQL result using only the data below.\n",
    "Do not invent values.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Data:\n",
    "{json.dumps(data, indent=2)}\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.chat.completions.create(\n",
    "        model=\"databricks-gpt-oss-120b\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a SQL and analytics assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "def run_sql(self, query: str) -> pd.DataFrame:\n",
    "        with sql.connect(\n",
    "            server_hostname=\"dbc-741c7540-6155.cloud.databricks.com\",\n",
    "            http_path=\"/sql/1.0/warehouses/e2eb5bcf69add002\",\n",
    "            access_token=os.environ[\"DATABRICKS_TOKEN\"],\n",
    "        ) as conn:\n",
    "            return pd.read_sql(query, conn)\n",
    "# -------------------------\n",
    "# 5️⃣ SQL execution with auto-repair\n",
    "# -------------------------\n",
    "def execute_sql(sql_query: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Execute SQL using Databricks SQL connector and return Pandas DataFrame.\n",
    "    Auto-repair logic can be added here later.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return run_sql(sql_query)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"SQL execution failed:\\n{sql_query}\\n\\n{e}\")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 6️⃣ PyFunc model\n",
    "# -------------------------\n",
    "import mlflow.pyfunc\n",
    "\n",
    "class PizzaRAGPyFuncModel(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        self.registry = REGISTRY\n",
    "        self.agent = SQLAgent(self.registry)\n",
    "        self.llm = get_llm()\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        query_text = str(model_input[\"query\"].iloc[0])\n",
    "\n",
    "        # Resolve intent(s) — can be single dict or list of dicts\n",
    "        resolved = resolve_intent(self.llm, query_text)\n",
    "        intents = resolved if isinstance(resolved, list) else [resolved]\n",
    "\n",
    "        all_answers = []\n",
    "        sql_queries = []\n",
    "        for intent in intents:\n",
    "            # Generate SQL for each intent\n",
    "            sql_query = self.agent.generate_sql(intent)\n",
    "\n",
    "            # Execute SQL\n",
    "            df = execute_sql(sql_query)\n",
    "\n",
    "            # Interpret result\n",
    "            answer_text = interpret_result(self.llm, query_text, df)\n",
    "\n",
    "            all_answers.append({\n",
    "                \"answer\": answer_text,\n",
    "                \"intent\": json.dumps(intent),\n",
    "                \"sql\": sql_query,\n",
    "                \"num_rows\": len(df),\n",
    "                \"num_documents\": len(intent.get(\"joins\", []))\n",
    "            })\n",
    "\n",
    "        # Combine into single DataFrame for MLflow\n",
    "        return pd.DataFrame(all_answers)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 7️⃣ MLflow model signature & log\n",
    "# -------------------------\n",
    "signature = ModelSignature(\n",
    "    inputs=Schema([ColSpec(\"string\", \"query\")]),\n",
    "    outputs=Schema([\n",
    "        ColSpec(\"string\", \"answer\"),\n",
    "        ColSpec(\"string\", \"intent\"),\n",
    "        ColSpec(\"string\", \"sql\"),\n",
    "        ColSpec(\"long\", \"num_rows\"),\n",
    "        ColSpec(\"long\", \"num_documents\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.pyfunc.log_model(\n",
    "        name=\"pizza_rag_model\",\n",
    "        python_model=PizzaRAGPyFuncModel(),\n",
    "        signature=signature,\n",
    "        input_example={\"query\": \"how much revenue generated in 25 december and year 2015\"},\n",
    "        registered_model_name=\"retail_analytics.pizza.pizza_rag_models\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cac4b437-99a2-444a-b068-308e75bb5455",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "final code"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import yaml\n",
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from mlflow.models import ModelSignature\n",
    "from mlflow.types import Schema, ColSpec\n",
    "from databricks import sql\n",
    "import ast\n",
    "import re\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1️⃣ Registry YAML (embedded)\n",
    "# -------------------------\n",
    "REGISTRY_YAML = \"\"\"\n",
    "entities:\n",
    "  pizza:\n",
    "    grain: pizza_name_id\n",
    "    tables:\n",
    "      monthly: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "      daily: retail_analytics.pizza.gold_pizza_metrics_daily\n",
    "    time_dimensions: [day, month, year]\n",
    "\n",
    "  ingredient:\n",
    "    grain: ingredient\n",
    "    tables:\n",
    "      usage: retail_analytics.pizza.gold_ingredient_usage\n",
    "    time_dimensions: [month, year]\n",
    "\n",
    "# =========================\n",
    "# BASE METRICS\n",
    "# =========================\n",
    "metrics:\n",
    "  total_pizzas_sold:\n",
    "    entity: pizza\n",
    "    grains:\n",
    "      day:\n",
    "        table: retail_analytics.pizza.gold_pizza_metrics_daily\n",
    "        sql: SUM(total_pizzas_sold)\n",
    "      month:\n",
    "        table: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "        sql: SUM(total_pizzas_sold)\n",
    "      year:\n",
    "        table: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "        sql: SUM(total_pizzas_sold)\n",
    "\n",
    "  total_revenue:\n",
    "    sql: SUM(total_revenue)\n",
    "    table: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "    entity: pizza\n",
    "\n",
    "  daily_revenue:\n",
    "    sql: SUM(revenue_perday)\n",
    "    table: retail_analytics.pizza.gold_pizza_metrics_daily\n",
    "    entity: pizza\n",
    "\n",
    "  ingredient_usage_grams:\n",
    "    sql: SUM(total_ingredient_grams)\n",
    "    table: retail_analytics.pizza.gold_ingredient_usage\n",
    "    entity: ingredient\n",
    "\n",
    "  ingredient_items_qty_grams:\n",
    "    sql: SUM(items_qty_in_grams)\n",
    "    table: retail_analytics.pizza.gold_ingredient_usage\n",
    "    entity: ingredient\n",
    "\n",
    "# =========================\n",
    "# DERIVED METRICS\n",
    "# =========================\n",
    "derived_metrics:\n",
    "  average_daily_revenue:\n",
    "    sql: SUM(revenue_perday) / COUNT(DISTINCT day)\n",
    "    table: retail_analytics.pizza.gold_pizza_metrics_daily\n",
    "    entity: pizza\n",
    "\n",
    "  revenue_per_pizza:\n",
    "    sql: SUM(total_revenue) / SUM(total_pizzas_sold)\n",
    "    table: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "    entity: pizza\n",
    "\n",
    "  revenue_per_ingredient_gram:\n",
    "    sql: SUM(total_revenue) / SUM(total_ingredient_grams)\n",
    "    tables:\n",
    "      revenue: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "      ingredient: retail_analytics.pizza.gold_ingredient_usage\n",
    "    entity: ingredient\n",
    "\n",
    "# =========================\n",
    "# JOIN DEFINITIONS\n",
    "# =========================\n",
    "joins:\n",
    "  pizza_to_ingredient:\n",
    "    left_table: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "    right_table: retail_analytics.pizza.gold_ingredient_usage\n",
    "    on:\n",
    "      - pizza_name_id\n",
    "      - month\n",
    "      - year\n",
    "\n",
    "  pizza_daily_to_monthly:\n",
    "    left_table: retail_analytics.pizza.gold_pizza_metrics_daily\n",
    "    right_table: retail_analytics.pizza.gold_pizza_metrics_monthly\n",
    "    on:\n",
    "      - pizza_name_id\n",
    "      - month\n",
    "      - year\n",
    "\n",
    "# =========================\n",
    "# TIME AGGREGATIONS\n",
    "# =========================\n",
    "time_grains:\n",
    "  daily:\n",
    "    columns: [day, month, year]\n",
    "\n",
    "  monthly:\n",
    "    columns: [month, year]\n",
    "\n",
    "  yearly:\n",
    "    columns: [year]\n",
    "\n",
    "# =========================\n",
    "# RANKING & WINDOW PATTERNS\n",
    "# =========================\n",
    "ranking_patterns:\n",
    "  highest:\n",
    "    order: DESC\n",
    "    limit: 1\n",
    "\n",
    "  lowest:\n",
    "    order: ASC\n",
    "    limit: 1\n",
    "\n",
    "  top_n:\n",
    "    order: DESC\n",
    "    limit: N\n",
    "\n",
    "  bottom_n:\n",
    "    order: ASC\n",
    "    limit: N\n",
    "\n",
    "window_patterns:\n",
    "  rank_within_time:\n",
    "    sql: RANK() OVER (PARTITION BY {time} ORDER BY {metric} DESC)\n",
    "\n",
    "  dense_rank_within_time:\n",
    "    sql: DENSE_RANK() OVER (PARTITION BY {time} ORDER BY {metric} DESC)\n",
    "\n",
    "# =========================\n",
    "# TREND PATTERNS\n",
    "# =========================\n",
    "trend_patterns:\n",
    "  month_over_month_change:\n",
    "    sql: \"{metric} - LAG({metric}) OVER (PARTITION BY {entity} ORDER BY year, month)\"\n",
    "\n",
    "  month_over_month_growth_pct:\n",
    "    sql: \"({metric} - LAG({metric}) OVER (PARTITION BY {entity} ORDER BY year, month))\n",
    "          / NULLIF(LAG({metric}) OVER (PARTITION BY {entity} ORDER BY year, month), 0)\"\n",
    "\n",
    "  consecutive_decline:\n",
    "    rule: \"negative change for N consecutive months\"\n",
    "\n",
    "# =========================\n",
    "# BUSINESS TERM DEFINITIONS\n",
    "# =========================\n",
    "business_terms:\n",
    "  peak_month:\n",
    "    definition: \"month with highest total_revenue\"\n",
    "\n",
    "  bottleneck:\n",
    "    definition: \"ingredient with highest total_ingredient_grams during peak_month\"\n",
    "\n",
    "  least_used:\n",
    "    definition: \"minimum total_ingredient_grams\"\n",
    "\n",
    "  top_selling:\n",
    "    definition: \"highest total_pizzas_sold\"\n",
    "\n",
    "  volatile:\n",
    "    definition: \"highest standard deviation of monthly revenue\"\n",
    "\n",
    "  declining_sales:\n",
    "    definition: \"negative month_over_month_change for 3 consecutive months\"\n",
    "\n",
    "# =========================\n",
    "# VALIDATION RULES\n",
    "# =========================\n",
    "validation:\n",
    "  forbid_columns:\n",
    "    - peak\n",
    "    - best\n",
    "    - highest\n",
    "  require_time_for_trends: true\n",
    "  require_join_for_derived_metrics: true\"\"\"\n",
    "REGISTRY = yaml.safe_load(REGISTRY_YAML)\n",
    "\n",
    "# -------------------------\n",
    "# 2️⃣ SQLAgent\n",
    "# -------------------------\n",
    "class SQLAgent:\n",
    "    def __init__(self, registry):\n",
    "        self.registry = registry\n",
    "\n",
    "    def plan(self, intent):\n",
    "        return intent\n",
    "\n",
    "    def generate_sql(self, intent):\n",
    "        entity = intent[\"entity\"]\n",
    "        metric = intent[\"metric\"]\n",
    "        time_grain = intent.get(\"time_grain\")  # day / month / year / None\n",
    "        window = intent.get(\"window\")\n",
    "        trend = intent.get(\"trend\")\n",
    "        ranking = intent.get(\"ranking\")\n",
    "        joins = intent.get(\"joins\", [])\n",
    "        filters = intent.get(\"filters\", [])\n",
    "\n",
    "        # -----------------------------\n",
    "        # 1️⃣ Determine base table\n",
    "        # -----------------------------\n",
    "        # Use daily table for day-level queries, monthly for month/year\n",
    "        # -----------------------------\n",
    "        # 1️⃣ Determine base table (METRIC-DRIVEN)\n",
    "        # -----------------------------\n",
    "        # metric_cfg = self.registry[\"metrics\"][metric]\n",
    "        # metric_entity = metric_cfg[\"entity\"]\n",
    "        # if intent.get(\"time_grain\") == \"day\" or intent.get(\"breakdown\") == \"day\":\n",
    "        #     table = \"retail_analytics.pizza.gold_pizza_metrics_daily\"\n",
    "        # else:\n",
    "        #     table = \"retail_analytics.pizza.gold_pizza_metrics_monthly\"\n",
    "        # if metric_entity == \"ingredient\":\n",
    "        #     base_table = self.registry[\"entities\"][\"ingredient\"][\"tables\"][\"usage\"]\n",
    "        # else:\n",
    "        #     # pizza metrics\n",
    "        #     if time_grain == \"day\":\n",
    "        #         base_table = self.registry[\"entities\"][\"pizza\"][\"tables\"][\"daily\"]\n",
    "        #     else:\n",
    "        #         base_table = self.registry[\"entities\"][\"pizza\"][\"tables\"][\"monthly\"]\n",
    "                # -----------------------------\n",
    "        # 1️⃣ Metric-driven grain resolution (SINGLE SOURCE OF TRUTH)\n",
    "        # -----------------------------\n",
    "        metric_cfg = self.registry[\"metrics\"][metric]\n",
    "\n",
    "        # Determine effective grain\n",
    "        effective_grain = (\n",
    "            intent.get(\"breakdown\")\n",
    "            if intent.get(\"breakdown\") in [\"day\", \"month\", \"year\"]\n",
    "            else intent.get(\"time_grain\")\n",
    "        )\n",
    "\n",
    "        # Default grain if still None\n",
    "        if effective_grain is None:\n",
    "            effective_grain = \"month\"\n",
    "\n",
    "        # Case 1: Grain-aware metric\n",
    "        if \"grains\" in metric_cfg:\n",
    "            grains_cfg = metric_cfg[\"grains\"]\n",
    "\n",
    "            if effective_grain not in grains_cfg:\n",
    "                raise ValueError(\n",
    "                    f\"Metric '{metric}' does not support grain '{effective_grain}'\"\n",
    "                )\n",
    "\n",
    "            grain_cfg = grains_cfg[effective_grain]\n",
    "            base_table = grain_cfg[\"table\"]\n",
    "            metric_sql = grain_cfg[\"sql\"]\n",
    "\n",
    "# Case 2: Legacy flat metric (BACKWARD COMPATIBLE)\n",
    "        else:\n",
    "            base_table = metric_cfg[\"table\"]\n",
    "            metric_sql = metric_cfg[\"sql\"]\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2️⃣ Determine metric SQL\n",
    "        # -----------------------------\n",
    "        # if time_grain == \"day\" and metric in [\"total_revenue\", \"daily_revenue\"]:\n",
    "        #     metric_sql = \"SUM(revenue_perday)\"\n",
    "        # else:\n",
    "        #     metric_sql = self.registry[\"metrics\"][metric][\"sql\"]\n",
    "\n",
    " \n",
    "        # -----------------------------\n",
    "# 4️⃣ Build SELECT & GROUP BY\n",
    "# -----------------------------\n",
    "        select_cols = []\n",
    "        group_cols = []\n",
    "\n",
    "        breakdown = intent.get(\"breakdown\", \"none\")\n",
    "\n",
    "        # ---------- BREAKDOWN FIRST (OVERRIDES DEFAULT GRAIN) ----------\n",
    "        if breakdown == \"month\":\n",
    "            select_cols.extend([\"year\", \"month\"])\n",
    "            group_cols.extend([\"year\", \"month\"])\n",
    "            intent[\"__exclude_grain__\"] = True\n",
    "        elif breakdown == \"day\":\n",
    "            select_cols.extend([\"day\", \"month\", \"year\"])\n",
    "            group_cols.extend([\"day\", \"month\", \"year\"])\n",
    "            intent[\"__exclude_grain__\"] = True\n",
    "\n",
    "        elif breakdown == \"year\":\n",
    "            select_cols.append(\"year\")\n",
    "            group_cols.append(\"year\")\n",
    "            intent[\"__exclude_grain__\"] = True\n",
    "\n",
    "        elif breakdown == \"pizza\":\n",
    "            select_cols.append(\"pizza_name_id\")\n",
    "            group_cols.append(\"pizza_name_id\")\n",
    "\n",
    "        elif breakdown == \"ingredient\":\n",
    "            select_cols.append(\"ingredient\")\n",
    "            group_cols.append(\"ingredient\")\n",
    "\n",
    "        else:\n",
    "            # no breakdown → pure aggregate\n",
    "            intent[\"__exclude_grain__\"] = True\n",
    "\n",
    "\n",
    "        # ---------- TIME GRAIN (ONLY IF NO BREAKDOWN) ----------\n",
    "        time_grain = intent.get(\"time_grain\")\n",
    "\n",
    "        if breakdown == \"none\":\n",
    "            if time_grain == \"day\":\n",
    "                select_cols += [\"day\", \"month\", \"year\"]\n",
    "                group_cols += [\"day\", \"month\", \"year\"]\n",
    "\n",
    "            elif time_grain == \"month\":\n",
    "                select_cols += [\"month\", \"year\"]\n",
    "                group_cols += [\"month\", \"year\"]\n",
    "\n",
    "            elif time_grain == \"year\":\n",
    "                select_cols += [\"year\"]\n",
    "                group_cols += [\"year\"]\n",
    "\n",
    "\n",
    "        # ---------- DEDUP ----------\n",
    "        select_cols = list(dict.fromkeys(select_cols))\n",
    "        group_cols = list(dict.fromkeys(group_cols))\n",
    "\n",
    "        # -----------------------------\n",
    "        # 5️⃣ Build JOINs\n",
    "        # -----------------------------\n",
    "\n",
    "        join_sql = []\n",
    "        for j in joins:\n",
    "            if isinstance(j, str):\n",
    "                # Join from registry\n",
    "                cfg = self.registry[\"joins\"][j]\n",
    "                join_sql.append(f\"JOIN {cfg['right_table']} USING ({', '.join(cfg['on'])})\")\n",
    "            elif isinstance(j, dict):\n",
    "                # Join specified as dict\n",
    "                right_table = j.get(\"right\") or j.get(\"right_table\")\n",
    "                on_cols = j.get(\"on\", [])\n",
    "                if right_table and on_cols:\n",
    "                    join_sql.append(f\"JOIN {right_table} USING ({', '.join(on_cols)})\")\n",
    "\n",
    "\n",
    "        # -----------------------------\n",
    "        # 6️⃣ Build WHERE filters\n",
    "        # -----------------------------\n",
    " \n",
    "        filter_clauses = []\n",
    "        MONTH_MAP = {\n",
    "                    \"january\": 1, \"february\": 2, \"march\": 3,\n",
    "                    \"april\": 4, \"may\": 5, \"june\": 6,\n",
    "                    \"july\": 7, \"august\": 8, \"september\": 9,\n",
    "                    \"october\": 10, \"november\": 11, \"december\": 12\n",
    "                            }\n",
    "        for f in filters:\n",
    "            field = f.get(\"field\")\n",
    "            op = f.get(\"operator\", \"=\")\n",
    "            value = f.get(\"value\")\n",
    "\n",
    "            # convert 'eq' to '='\n",
    "            if op.lower() == \"eq\":\n",
    "                op = \"=\"\n",
    "            if field == \"month\" and isinstance(value, str):\n",
    "                value = MONTH_MAP.get(value.lower())\n",
    "                if value is None:\n",
    "                    raise ValueError(f\"Invalid month value: {f.get('value')}\")\n",
    "            # handle 'date' filter for pizza daily\n",
    "            if field == \"date\":\n",
    "                if time_grain == \"day\":\n",
    "                    yyyy, mm, dd = value.split(\"-\")\n",
    "                    filter_clauses.append(f\"day = {int(dd)}\")\n",
    "                    filter_clauses.append(f\"month = {int(mm)}\")\n",
    "                    filter_clauses.append(f\"year = {int(yyyy)}\")\n",
    "                elif time_grain == \"month\":\n",
    "                    yyyy, mm = value.split(\"-\")\n",
    "                    filter_clauses.append(f\"month = {int(mm)}\")\n",
    "                    filter_clauses.append(f\"year = {int(yyyy)}\")\n",
    "                elif time_grain == \"year\":\n",
    "                    yyyy = value.split(\"-\")[0]\n",
    "                    filter_clauses.append(f\"year = {int(yyyy)}\")\n",
    "            elif field == \"year\":\n",
    "                filter_clauses.append(f\"year {op} {value}\")\n",
    "            else:\n",
    "                if isinstance(value, str):\n",
    "                    value_sql = f\"'{value}'\"\n",
    "                else:\n",
    "                    value_sql = str(value)\n",
    "                filter_clauses.append(f\"{field} {op} {value_sql}\")\n",
    "        filter_clauses = apply_time_range(intent, filter_clauses)\n",
    "        where_sql = f\"WHERE {' AND '.join(filter_clauses)}\" if filter_clauses else \"\"\n",
    "\n",
    "\n",
    "        # -----------------------------\n",
    "        # 7️⃣ Base CTE\n",
    "        # -----------------------------\n",
    "        sql = f\"\"\"\n",
    "WITH base AS (\n",
    "    SELECT\n",
    "        {', '.join(select_cols)},\n",
    "        {metric_sql} AS metric_value\n",
    "    FROM {base_table}\n",
    "    {' '.join(join_sql)}\n",
    "    {where_sql}\n",
    "    GROUP BY {', '.join(group_cols)}\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "        # -----------------------------\n",
    "        # 8️⃣ Apply window function\n",
    "        # -----------------------------\n",
    "        final_cte = \"base\"\n",
    "        if window:\n",
    "            window_sql = self.registry[\"window_patterns\"][window][\"sql\"].format(\n",
    "                time=\",\".join(group_cols),\n",
    "                metric=\"metric_value\"\n",
    "            )\n",
    "            sql += f\"\"\"\n",
    ", ranked AS (\n",
    "    SELECT *,\n",
    "           {window_sql} AS rank\n",
    "    FROM base\n",
    ")\n",
    "\"\"\"\n",
    "            final_cte = \"ranked\"\n",
    "\n",
    "        # -----------------------------\n",
    "        # 9️⃣ Apply trend function\n",
    "        # -----------------------------\n",
    "        if trend:\n",
    "            trend_sql = self.registry[\"trend_patterns\"][trend][\"sql\"].format(\n",
    "                entity=grain_col,\n",
    "                metric=\"metric_value\",\n",
    "                time=\",\".join(group_cols)\n",
    "            )\n",
    "            sql += f\"\"\"\n",
    ", trended AS (\n",
    "    SELECT *,\n",
    "           {trend_sql} AS trend_value\n",
    "    FROM base\n",
    ")\n",
    "\"\"\"\n",
    "            final_cte = \"trended\"\n",
    "\n",
    "        # -----------------------------\n",
    "        # 10️⃣ Final SELECT + ranking\n",
    "        # -----------------------------\n",
    "        sql += f\"SELECT * FROM {final_cte}\\n\"\n",
    "        ranking_type = None\n",
    "        ranking_limit = None\n",
    "\n",
    "        if isinstance(ranking, dict):\n",
    "            ranking_type = ranking.get(\"type\")\n",
    "            ranking_limit = ranking.get(\"limit\")\n",
    "\n",
    "        elif isinstance(ranking, str):\n",
    "            ranking_type = ranking\n",
    "\n",
    "        # Default limits\n",
    "        if ranking_type in (\"highest\", \"lowest\") and ranking_limit is None:\n",
    "            ranking_limit = 1\n",
    "\n",
    "        # Resolve pattern from registry\n",
    "        if ranking_type:\n",
    "            pattern = self.registry[\"ranking_patterns\"].get(ranking_type)\n",
    "            if not pattern:\n",
    "                raise ValueError(f\"Unknown ranking type: {ranking_type}\")\n",
    "\n",
    "            order_dir = pattern[\"order\"]\n",
    "            sql += f\"ORDER BY metric_value {order_dir}\\n\"\n",
    "\n",
    "        # Apply LIMIT\n",
    "        if ranking_limit:\n",
    "            sql += f\"LIMIT {int(ranking_limit)}\\n\"\n",
    "        return sql\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def apply_time_range(intent, filter_clauses):\n",
    "    tr = intent.get(\"time_range\")\n",
    "    if not tr:\n",
    "        return filter_clauses\n",
    "\n",
    "    now = datetime.now()\n",
    "    current_month = now.month\n",
    "    current_year = now.year\n",
    "\n",
    "    ttype = tr.get(\"type\")\n",
    "\n",
    "    if ttype in (\"quarter\", \"half\", \"range\"):\n",
    "        start = tr[\"start_month\"]\n",
    "        end = tr[\"end_month\"]\n",
    "        filter_clauses.append(f\"month BETWEEN {start} AND {end}\")\n",
    "\n",
    "    elif ttype == \"ytd\":\n",
    "        filter_clauses.append(f\"month BETWEEN 1 AND {current_month}\")\n",
    "        filter_clauses.append(f\"year = {current_year}\")\n",
    "\n",
    "    elif ttype == \"ltm\":\n",
    "        # Rolling 12 months\n",
    "        filter_clauses.append(\n",
    "            f\"(year = {current_year} AND month <= {current_month}) OR \"\n",
    "            f\"(year = {current_year - 1} AND month > {current_month})\"\n",
    "        )\n",
    "\n",
    "    # IMPORTANT: drop grouping\n",
    "    intent[\"__exclude_grain__\"] = True\n",
    "    intent[\"time_grain\"] = None\n",
    "\n",
    "    return filter_clauses\n",
    "\n",
    "# -------------------------\n",
    "# 3️⃣ LLM client helper (works notebook + serving)\n",
    "# -------------------------\n",
    "def get_llm():\n",
    "    return OpenAI(\n",
    "        base_url=f\"{os.environ['DATABRICKS_HOST']}/serving-endpoints\",\n",
    "        api_key=os.environ[\"DATABRICKS_TOKEN\"],\n",
    "    )\n",
    "\n",
    "def extract_intent_json(raw):\n",
    "    \"\"\"\n",
    "    Extract valid intent JSON from raw LLM output.\n",
    "\n",
    "    Handles:\n",
    "    - list outputs from Databricks LLM\n",
    "    - extra reasoning text before JSON\n",
    "    - JSON array or single JSON object\n",
    "    \"\"\"\n",
    "    # --- Step 1: Convert list outputs to string ---\n",
    "    if isinstance(raw, list):\n",
    "        texts = []\n",
    "        for item in raw:\n",
    "            if isinstance(item, dict) and \"text\" in item:\n",
    "                texts.append(item[\"text\"])\n",
    "            else:\n",
    "                texts.append(str(item))\n",
    "        raw = \"\".join(texts)\n",
    "        print(raw)\n",
    "\n",
    "    # --- Step 2: Extract all JSON objects or arrays using regex ---\n",
    "    matches = re.findall(r'(\\{.*?\\}|\\[.*?\\])', raw, flags=re.DOTALL)\n",
    "    if not matches:\n",
    "        raise ValueError(f\"No JSON found in LLM output:\\n{raw}\")\n",
    "\n",
    "    # --- Step 3: Pick last valid JSON containing 'entity' ---\n",
    "    for m in reversed(matches):\n",
    "        try:\n",
    "            candidate = json.loads(m)\n",
    "            # Accept dict or list of dicts with 'entity' key\n",
    "            if isinstance(candidate, dict) and \"entity\" in candidate:\n",
    "                return candidate\n",
    "            if isinstance(candidate, list) and all(isinstance(c, dict) and \"entity\" in c for c in candidate):\n",
    "                return candidate\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    raise ValueError(f\"No valid intent JSON found in LLM output:\\n{raw}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3️⃣ Resolve intent safely (single intent)\n",
    "# -------------------------\n",
    "def resolve_intent(llm, question: str) -> dict:\n",
    "    DEFAULT_INTENT = {\n",
    "    \"entity\": \"pizza\",\n",
    "    \"metric\": \"total_revenue\",\n",
    "    \"time_grain\": None,\n",
    "    \"window\": None,\n",
    "    \"trend\": None,\n",
    "    \"ranking\": None,\n",
    "    \"joins\": [],\n",
    "    \"filters\": [],\n",
    "    \"time_range\": None\n",
    "    }\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an analytics intent parser.\n",
    "\n",
    "You MUST return ONLY valid JSON.\n",
    "No markdown.\n",
    "No explanations.\n",
    "No extra text.\n",
    "\n",
    "====================\n",
    "OUTPUT SCHEMA\n",
    "====================\n",
    "{{\n",
    "  \"entity\": \"pizza | ingredient\",\n",
    "  \"metric\": \"total_revenue | total_pizzas_sold | ingredient_usage_grams\",\n",
    "  \"time_grain\": \"day | month | year | null\",\n",
    "  \"window\": \"rank_within_time | null\",\n",
    "  \"trend\": \"month_over_month_growth_pct | null\",\n",
    "  \"breakdown\": \"none | month | year | pizza | ingredient\"\n",
    "  \"ranking\": null | {{\n",
    "    \"type\": \"highest | lowest | top_n | bottom_n\",\n",
    "    \"limit\": number\n",
    "  }},\n",
    "  \"joins\": [],\n",
    "  \"filters\": [],\n",
    "  \"time_range\": null | {{\n",
    "    \"type\": \"quarter | half | range | ytd | ltm\",\n",
    "    \"start_month\": number | null,\n",
    "    \"end_month\": number | null\n",
    "  }}\n",
    "}}\n",
    "\n",
    "====================\n",
    "DECISION ORDER (MANDATORY)\n",
    "====================\n",
    "1. Identify entity and metric\n",
    "2. Detect business time periods (time_range)\n",
    "3. IF time_range exists:\n",
    "   - DO NOT create month filters\n",
    "   - Set time_grain = null\n",
    "4. ELSE:\n",
    "   - Extract explicit month / year / date filters\n",
    "5. Then detect ranking, trend, window\n",
    "\n",
    "====================\n",
    "TIME RANGE DETECTION (HIGHEST PRIORITY)\n",
    "====================\n",
    "Convert business periods to months:\n",
    "\n",
    "- Q1 / first quarter → 1–3\n",
    "- Q2 → 4–6\n",
    "- Q3 → 7–9\n",
    "- Q4 → 10–12\n",
    "- first half / H1 / first 6 months → 1–6\n",
    "- second half / H2 → 7–12\n",
    "- YTD / year to date → 1 to current month\n",
    "- LTM / last 12 months / trailing 12 months → rolling 12 months\n",
    "\n",
    "Rules:\n",
    "- If any time_range is detected:\n",
    "  - Populate time_range\n",
    "  - filters MUST be empty\n",
    "  - time_grain MUST be null\n",
    "\n",
    "====================\n",
    "FILTER RULES (ONLY IF NO TIME_RANGE)\n",
    "====================\n",
    "- Month names → month filter (string)\n",
    "- Year numbers → year filter (number)\n",
    "- Exact dates → field = \"date\" (YYYY-MM-DD)\n",
    "- filters MUST always be a list\n",
    "\n",
    "Filter format:\n",
    "{{\n",
    "  \"field\": \"month | year | date | pizza_name_id | ingredient\",\n",
    "  \"operator\": \"=\",\n",
    "  \"value\": string | number\n",
    "}}\n",
    "====================\n",
    "BREAKDOWN RULES (MANDATORY)\n",
    "====================\n",
    "You MUST decide the breakdown dimension based on the user's question.\n",
    "\n",
    "Examples:\n",
    "- \"monthly data\", \"per month\", \"month-wise\", \"trend across months\"\n",
    "  → breakdown = \"month\"\n",
    "\n",
    "- \"by pizza\", \"top pizzas\", \"which pizza\"\n",
    "  → breakdown = \"pizza\"\n",
    "\n",
    "- \"ingredient usage by ingredient\"\n",
    "  → breakdown = \"ingredient\"\n",
    "\n",
    "- If user asks only for a total (no breakdown wording)\n",
    "  → breakdown = \"none\"\n",
    "\n",
    "If breakdown is not \"none\":\n",
    "- DO NOT include entity grain unless breakdown explicitly requests it\n",
    "====================\n",
    "RANKING RULES\n",
    "====================\n",
    "- most / highest / best → {{ \"type\": \"highest\" }}\n",
    "- least / lowest → {{ \"type\": \"lowest\" }}\n",
    "- top N → {{ \"type\": \"top_n\", \"limit\": N }}\n",
    "- bottom N → {{ \"type\": \"bottom_n\", \"limit\": N }}\n",
    "\n",
    "====================\n",
    "EXAMPLES\n",
    "====================\n",
    "Question: how many pizzas sold in first 6 months\n",
    "Output:\n",
    "{{\n",
    "  \"entity\": \"pizza\",\n",
    "  \"metric\": \"total_pizzas_sold\",\n",
    "  \"time_grain\": null,\n",
    "  \"window\": null,\n",
    "  \"trend\": null,\n",
    "  \"ranking\": null,\n",
    "  \"joins\": [],\n",
    "  \"filters\": [],\n",
    "  \"time_range\": {{\n",
    "    \"type\": \"half\",\n",
    "    \"start_month\": 1,\n",
    "    \"end_month\": 6\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Question: top 3 pizzas by revenue in December 2015\n",
    "Output:\n",
    "{{\n",
    "  \"entity\": \"pizza\",\n",
    "  \"metric\": \"total_revenue\",\n",
    "  \"time_grain\": \"month\",\n",
    "  \"window\": null,\n",
    "  \"trend\": null,\n",
    "  \"ranking\": {{ \"type\": \"top_n\", \"limit\": 3 }},\n",
    "  \"joins\": [],\n",
    "  \"filters\": [\n",
    "    {{ \"field\": \"month\", \"operator\": \"=\", \"value\": \"december\" }},\n",
    "    {{ \"field\": \"year\", \"operator\": \"=\", \"value\": 2015 }}\n",
    "  ],\n",
    "  \"time_range\": null\n",
    "}}\n",
    "\n",
    "====================\n",
    "QUESTION\n",
    "====================\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.chat.completions.create(\n",
    "        model=\"databricks-gpt-oss-120b\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You output ONLY valid JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    raw = response.choices[0].message.content\n",
    "\n",
    "    # --- Parse LLM output safely ---\n",
    "    if isinstance(raw, dict):\n",
    "        intents = [raw]\n",
    "    elif isinstance(raw, str):\n",
    "        parsed = json.loads(raw)\n",
    "        # Ensure it’s a list of intents\n",
    "        intents = parsed if isinstance(parsed, list) else [parsed]\n",
    "    elif isinstance(raw, list):\n",
    "        # Databricks OSS output list format\n",
    "        texts = [x[\"text\"] for x in raw if isinstance(x, dict) and \"text\" in x]\n",
    "        combined_text = \"\".join(texts)\n",
    "        parsed = json.loads(combined_text)\n",
    "        intents = parsed if isinstance(parsed, list) else [parsed]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported LLM output type: {type(raw)}\")\n",
    "\n",
    "    # --- Take only the first intent to avoid list issues ---\n",
    "    intent = DEFAULT_INTENT.copy()\n",
    "    intent.update(intents[0])\n",
    "    q = question.lower()\n",
    "\n",
    "    BUSINESS_TIME_PHRASES = [\n",
    "        \"worst month\",\n",
    "        \"best month\",\n",
    "        \"worst year\",\n",
    "        \"best year\",\n",
    "        \"business performance\",\n",
    "        \"overall performance\"\n",
    "    ]\n",
    "\n",
    "    if any(p in q for p in BUSINESS_TIME_PHRASES):\n",
    "        # Force business-level aggregation\n",
    "        intent[\"__exclude_grain__\"] = True\n",
    "\n",
    "        # These questions are always time-based\n",
    "        if intent.get(\"time_grain\") is None:\n",
    "            intent[\"time_grain\"] = \"month\"\n",
    "\n",
    "        # Ranking default for worst/best\n",
    "        if \"worst\" in q and not intent.get(\"ranking\"):\n",
    "            intent[\"ranking\"] = {\"type\": \"lowest\"}\n",
    "        elif \"best\" in q and not intent.get(\"ranking\"):\n",
    "            intent[\"ranking\"] = {\"type\": \"highest\"}\n",
    "    date_match = re.search(r'\\b(january|february|march|april|may|june|july|august|september|october|november|december)\\s+\\d{1,2}\\b', q)\n",
    "\n",
    "    if date_match:\n",
    "        intent[\"time_grain\"] = \"day\"\n",
    "        intent[\"breakdown\"] = \"day\"\n",
    "        month_name, day = date_match.group().split()\n",
    "        month_num = datetime.strptime(month_name, \"%B\").month\n",
    "\n",
    "        intent[\"filters\"].append({\n",
    "            \"field\": \"month\",\n",
    "            \"operator\": \"=\",\n",
    "            \"value\": month_num\n",
    "        })\n",
    "\n",
    "        intent[\"filters\"].append({\n",
    "            \"field\": \"day\",\n",
    "            \"operator\": \"=\",\n",
    "            \"value\": int(day)\n",
    "        })\n",
    "    return intent\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4️⃣ Result interpreter\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "def interpret_result(llm, question: str, df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    ALWAYS returns a plain string from LLM output.\n",
    "    Handles OSS model output that can be list/dict/text.\n",
    "    \"\"\"\n",
    "\n",
    "    data_preview = df.head(50).to_dict(orient=\"records\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a data analyst.\n",
    "\n",
    "Rules:\n",
    "- Answer the question using ONLY the data provided.\n",
    "- Do NOT invent values.\n",
    "- Do NOT show reasoning or JSON.\n",
    "- Output ONLY a single plain English paragraph.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Data:\n",
    "{data_preview}\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.chat.completions.create(\n",
    "        model=\"databricks-gpt-oss-120b\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You output ONLY plain text.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # 🔹 Extract text from all possible OSS formats\n",
    "    raw = response.choices[0].message.content\n",
    "\n",
    "    # If it's a list of dicts (Databricks OSS default)\n",
    "    if isinstance(raw, list):\n",
    "        texts = []\n",
    "        for item in raw:\n",
    "            if isinstance(item, dict) and \"text\" in item:\n",
    "                texts.append(item[\"text\"])\n",
    "            else:\n",
    "                texts.append(str(item))\n",
    "        raw = \" \".join(texts)\n",
    "\n",
    "    # If it's a dict, try common fields\n",
    "    elif isinstance(raw, dict):\n",
    "        if \"text\" in raw:\n",
    "            raw = raw[\"text\"]\n",
    "        elif \"summary\" in raw and isinstance(raw[\"summary\"], list):\n",
    "            raw = \" \".join([s.get(\"text\", \"\") for s in raw[\"summary\"]])\n",
    "        else:\n",
    "            raw = str(raw)\n",
    "\n",
    "    # Force string type as a last resort\n",
    "    if not isinstance(raw, str):\n",
    "        raw = str(raw)\n",
    "\n",
    "    # Strip leading/trailing spaces\n",
    "    return raw.strip()\n",
    "\n",
    "def run_sql(query: str) -> pd.DataFrame:\n",
    "        with sql.connect(\n",
    "            server_hostname=\"dbc-741c7540-6155.cloud.databricks.com\",\n",
    "            http_path=\"/sql/1.0/warehouses/e2eb5bcf69add002\",\n",
    "            access_token=os.environ[\"DATABRICKS_TOKEN\"],\n",
    "        ) as conn:\n",
    "            return pd.read_sql(query, conn)\n",
    "# -------------------------\n",
    "# 5️⃣ SQL execution with auto-repair\n",
    "# -------------------------\n",
    "def execute_sql(sql_query: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Execute SQL using Databricks SQL connector and return Pandas DataFrame.\n",
    "    Auto-repair logic can be added here later.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(sql_query)\n",
    "        return run_sql(sql_query)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"SQL execution failed:\\n{sql_query}\\n\\n{e}\")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 6️⃣ PyFunc model\n",
    "# -------------------------\n",
    "import mlflow.pyfunc\n",
    "\n",
    "class PizzaRAGPyFuncModel(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        self.registry = REGISTRY\n",
    "        self.agent = SQLAgent(self.registry)\n",
    "        self.llm = get_llm()\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "      # -------------------------\n",
    "      # Extract query safely\n",
    "      # -------------------------\n",
    "      if isinstance(model_input, pd.DataFrame):\n",
    "          query_text = str(model_input.iloc[0][\"query\"])\n",
    "      elif isinstance(model_input, dict):\n",
    "          query_text = str(model_input.get(\"query\"))\n",
    "      elif isinstance(model_input, str):\n",
    "          query_text = model_input\n",
    "      else:\n",
    "          raise ValueError(f\"Unsupported model_input type: {type(model_input)}\")\n",
    "\n",
    "      # -------------------------\n",
    "      # Resolve intent\n",
    "      # -------------------------\n",
    "      intent = resolve_intent(self.llm, query_text)\n",
    "      intent[\"entity\"] = self.registry[\"metrics\"][intent[\"metric\"]][\"entity\"]\n",
    "\n",
    "      q = query_text.lower()\n",
    "\n",
    "      if \"which month\" in q or \"by month\" in q:\n",
    "          intent[\"time_grain\"] = \"month\"\n",
    "          intent[\"entity\"] = \"pizza\"\n",
    "          intent[\"__exclude_grain__\"] = True\n",
    "\n",
    "      # -------------------------\n",
    "      # Generate & execute SQL\n",
    "      # -------------------------\n",
    "      sql_query = self.agent.generate_sql(intent)\n",
    "      df = execute_sql(sql_query)\n",
    "\n",
    "      # -------------------------\n",
    "      # Interpret result\n",
    "      # 🔥 CRITICAL FIX HERE\n",
    "      # -------------------------\n",
    "      raw_answer = interpret_result(self.llm, query_text, df)\n",
    "\n",
    "      # ✅ FORCE STRING OUTPUT\n",
    "      if isinstance(raw_answer, dict):\n",
    "          # Prefer readable text if present\n",
    "          if \"summary\" in raw_answer:\n",
    "              answer_text = \" \".join(\n",
    "                  s.get(\"text\", \"\") for s in raw_answer.get(\"summary\", [])\n",
    "              )\n",
    "          else:\n",
    "              answer_text = json.dumps(raw_answer)\n",
    "      else:\n",
    "          answer_text = str(raw_answer)\n",
    "\n",
    "      # 🚨 HARD LIMIT to avoid any future issues\n",
    "      answer_text = answer_text[:4000]\n",
    "\n",
    "      # -------------------------\n",
    "      # Return dataframe (simple types ONLY)\n",
    "      # -------------------------\n",
    "      return pd.DataFrame([{\n",
    "          \"answer\": answer_text,          # ✅ plain string\n",
    "          \"intent\": json.dumps(intent),   # ✅ string\n",
    "          \"sql\": sql_query,               # ✅ string\n",
    "          \"num_rows\": int(len(df)),\n",
    "          \"num_documents\": int(len(intent.get(\"joins\", [])))\n",
    "      }])\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 7️⃣ MLflow model signature & log\n",
    "# -------------------------\n",
    "signature = ModelSignature(\n",
    "    inputs=Schema([ColSpec(\"string\", \"query\")]),\n",
    "    outputs=Schema([\n",
    "        ColSpec(\"string\", \"answer\"),\n",
    "        ColSpec(\"string\", \"intent\"),\n",
    "        ColSpec(\"string\", \"sql\"),\n",
    "        ColSpec(\"long\", \"num_rows\"),\n",
    "        ColSpec(\"long\", \"num_documents\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.pyfunc.log_model(\n",
    "        name=\"analytics_pizza_rag_model\",\n",
    "        python_model=PizzaRAGPyFuncModel(),\n",
    "        signature=signature,\n",
    "        input_example={\"query\": \"in which month pizza sale was high in first quarter\"},\n",
    "        registered_model_name=\"retail_analytics.pizza.analytics_pizza_rag_models\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c91586f5-a984-43d0-8858-43f1d0ac93c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42d472c9-dce2-4a32-a294-b91ec5b0911d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 15"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Sample model output (stringified dict inside 'answer')\n",
    "sample_prediction = {\n",
    "    \"answer\": \"{'type': 'reasoning', 'summary': [{'type': 'summary_text', 'text': 'We need to answer: \\\"hoe many pizza sold in first 6 months\\\". Data is a list of pizza entries with metric_value. Likely each entry is a month? But no month info. Possibly first 6 entries correspond to first 6 months. So sum metric_value of first six items. Let\\\\'s compute: first six entries:\\\\n\\\\n1. ckn_alfredo_l: 95\\\\n2. spinach_fet_s: 245\\\\n3. soppressata_l: 222\\\\n4. ckn_pesto_s: 148\\\\n5. ital_supr_l: 368\\\\n6. green_garden_l: 53\\\\n\\\\nSum: 95+245=340; +222=562; +148=710; +368=1078; +53=1131.\\\\n\\\\nThus answer: 1,131 pizzas sold in first six months.'}]} A total of 1,131 pizzas were sold in the first six months.\\\"\"\n",
    "}\n",
    "\n",
    "def extract_final_answer(answer_raw: str) -> str:\n",
    "    if not isinstance(answer_raw, str):\n",
    "        return str(answer_raw)\n",
    "\n",
    "    text = answer_raw.strip()\n",
    "\n",
    "    # If reasoning block exists, take everything AFTER last closing brace\n",
    "    if \"}\" in text:\n",
    "        after_reasoning = text.rsplit(\"}\", 1)[-1].strip()\n",
    "        if after_reasoning:\n",
    "            return after_reasoning\n",
    "\n",
    "    # Fallback: return last sentence\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    return sentences[-1].strip()\n",
    "\n",
    "\n",
    "# Test\n",
    "clean_answer = extract_final_answer(sample_prediction)\n",
    "print(\"Extracted answer:\\n\")\n",
    "print(clean_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70afdbfe-253a-4bcc-b827-3b6bbdd414a6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dynamic extract_answer_text function"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "sample_prediction = {\n",
    "  \"predictions\": [\n",
    "    {\n",
    "      \"answer\": \"{'type': 'reasoning', 'summary': [{'type': 'summary_text', 'text': \\\"We need total revenue in May. Sum all metric_value entries. Let's sum.\\\\n\\\\nI'll add sequentially.\\\\n\\\\nList values:\\\\n\\\\n318.75\\\\n708.0\\\\n182.25\\\\n336.0\\\\n731.25\\\\n567.0\\\\n311.25\\\\n1514.75\\\\n2427.75\\\\n828.75\\\\n788.5\\\\n448.0\\\\n477.0\\\\n408.0\\\\n287.5\\\\n684.75\\\\n800.0\\\\n1203.5\\\\n1680.0\\\\n487.5\\\\n225.0\\\\n324.0\\\\n544.5\\\\n577.5\\\\n528.0\\\\n107.85000000000001\\\\n1113.25\\\\n159.25\\\\n528.0\\\\n156.0\\\\n720.0\\\\n1402.5\\\\n368.0\\\\n1377.0\\\\n452.25\\\\n1846.75\\\\n594.0\\\\n1306.5\\\\n518.75\\\\n630.0\\\\n1053.0\\\\n324.0\\\\n1086.5\\\\n462.0\\\\n951.75\\\\n1275.75\\\\n429.0\\\\n140.25\\\\n242.25\\\\n770.5\\\\n\\\\nNow sum. I'll do cumulative.\\\\n\\\\nStart 0\\\\n+318.75 = 318.75\\\\n+708.0 = 1026.75\\\\n+182.25 = 1209.0\\\\n+336.0 = 1545.0\\\\n+731.25 = 2276.25\\\\n+567.0 = 2843.25\\\\n+311.25 = 3154.5\\\\n+1514.75 = 4669.25\\\\n+2427.75 = 7097.0\\\\n+828.75 = 7925.75\\\\n+788.5 = 8714.25\\\\n+448.0 = 9162.25\\\\n+477.0 = 9639.25\\\\n+408.0 = 10047.25\\\\n+287.5 = 10334.75\\\\n+684.75 = 11019.5\\\\n+800.0 = 11819.5\\\\n+1203.5 = 13023.0\\\\n+1680.0 = 14703.0\\\\n+487.5 = 15190.5\\\\n+225.0 = 15415.5\\\\n+324.0 = 15739.5\\\\n+544.5 = 16284.0\\\\n+577.5 = 16861.5\\\\n+528.0 = 17389.5\\\\n+107.85000000000001 = 17497.350000000002 (approx 17497.35)\\\\n+1113.25 = 18610.600000000002\\\\n+159.25 = 18769.850000000002\\\\n+528.0 = 193... wait compute: 18769.85+528 = 19297.85? Actually 18769.85+528 = 19297.85. Yes.\\\\n+156.0 = 19453.85\\\\n+720.0 = 20173.85\\\\n+1402.5 = 21576.35\\\\n+368.0 = 21944.35\\\\n+1377.0 = 23321.35\\\\n+452.25 = 23773.6\\\\n+1846.75 = 25620.35\\\\n+594.0 = 26214.35\\\\n+1306.5 = 27520.85\\\\n+518.75 = 28039.6\\\\n+630.0 = 28669.6\\\\n+1053.0 = 29722.6\\\\n+324.0 = 30046.6\\\\n+1086.5 = 31133.1\\\\n+462.0 = 31595.1\\\\n+951.75 = 32546.85\\\\n+1275.75 = 33822.6\\\\n+429.0 = 34251.6\\\\n+140.25 = 34391.85\\\\n+242.25 = 34634.1\\\\n+770.5 = 35404.6\\\\n\\\\nTotal revenue May = 35,404.6 (assuming units). Provide answer.\\\"}]} The total revenue for May is 35,404.6.\",\n",
    "      \"intent\": \"{\\\"entity\\\": \\\"pizza\\\", \\\"metric\\\": \\\"total_revenue\\\", \\\"time_grain\\\": \\\"month\\\", \\\"window\\\": null, \\\"trend\\\": null, \\\"ranking\\\": null, \\\"joins\\\": [], \\\"filters\\\": [{\\\"field\\\": \\\"month\\\", \\\"operator\\\": \\\"=\\\", \\\"value\\\": \\\"may\\\"}], \\\"time_range\\\": null}\",\n",
    "      \"sql\": \"\\nWITH base AS (\\n    SELECT\\n        pizza_name_id, month, year,\\n        SUM(total_revenue) AS metric_value\\n    FROM retail_analytics.pizza.gold_pizza_metrics_monthly\\n    \\n    WHERE month = 5\\n    GROUP BY pizza_name_id, month, year\\n)\\nSELECT * FROM base\\n\",\n",
    "      \"num_rows\": 91,\n",
    "      \"num_documents\": 0\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "def get_dynamic_response(data):\n",
    "    try:\n",
    "        # 1. Access main components\n",
    "        prediction = data['predictions'][0]\n",
    "        full_text = prediction['answer']\n",
    "        \n",
    "        # 2. Parse the intent metadata\n",
    "        intent = json.loads(prediction.get('intent', '{}'))\n",
    "        metric = intent.get('metric', 'value').replace('_', ' ')\n",
    "        entity = intent.get('entity', '')\n",
    "        \n",
    "        # Extract filters (e.g., \"may\")\n",
    "        filters = intent.get('filters', [])\n",
    "        filter_context = \" for \" + \" \".join([f\"{f['value']}\" for f in filters]) if filters else \"\"\n",
    "\n",
    "        # 3. Dynamic Value Extraction\n",
    "        # Strategy A: If there's a ranking (Highest/Most used), look for the name after \"is\"\n",
    "        if \"ranking\" in intent and intent[\"ranking\"]:\n",
    "            # This regex finds the word immediately following \"is \" at the end of the sentence\n",
    "            match = re.search(r\"is ([A-Z][a-z]+|[\\w\\s]+)(?:\\.|\\\")?$\", full_text.strip())\n",
    "            if match:\n",
    "                value = match.group(1).strip()\n",
    "            else:\n",
    "                value = full_text.split(\"is\")[-1].strip().rstrip('.')\n",
    "        \n",
    "        # Strategy B: Look for bolded values (for revenue/counts)\n",
    "        elif \"**\" in full_text:\n",
    "            value = re.search(r'\\*\\*(.*?)\\*\\*', full_text).group(1)\n",
    "            \n",
    "        # Strategy C: Fallback to the last number in the text\n",
    "        else:\n",
    "            numbers = re.findall(r'[\\d,]+\\.?\\d*', full_text)\n",
    "            value = numbers[-1] if numbers else \"not found\"\n",
    "\n",
    "        # 4. Construct Sentence\n",
    "        if \"ranking\" in intent:\n",
    "            return f\"The most used {entity} for{filter_context} is {value}\"\n",
    "        else:\n",
    "            return f\"{metric.capitalize()}{filter_context} is {value}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error parsing: {e}\"\n",
    "\n",
    "# --- TEST CASE ---\n",
    "\n",
    "\n",
    "print(f\"Extracted: {get_dynamic_response(sample_prediction)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26cf56e4-1cbf-4979-8c2c-f5c8a2bc9446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_clean_answer(data):\n",
    "    try:\n",
    "        # 1. Access the 'answer' string\n",
    "        raw_text = data['predictions'][0]['answer']\n",
    "        \n",
    "        # 2. Find the position of the last closing brace '}' \n",
    "        # which marks the end of the reasoning block\n",
    "        last_brace_index = raw_text.rfind('}')\n",
    "        \n",
    "        if last_brace_index != -1:\n",
    "            # 3. Extract everything after the last brace\n",
    "            clean_answer = raw_text[last_brace_index + 1:].strip()\n",
    "            \n",
    "            # 4. Optional: Clean up any trailing quotes or artifacts\n",
    "            clean_answer = clean_answer.rstrip('\"').strip()\n",
    "            \n",
    "            return clean_answer\n",
    "            \n",
    "        return raw_text # Fallback if no brace found\n",
    "        \n",
    "    except (KeyError, IndexError, TypeError):\n",
    "        return \"Invalid JSON structure\"\n",
    "\n",
    "print(extract_clean_answer(sample_prediction))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sql_agent_rag_application",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
