{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0ff1cec-e35a-4100-8f58-78b486bde9c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cd6f72f-30ad-4d88-a1ab-b361c254da6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"year\",'')\n",
    "dbutils.widgets.text(\"month\",'')\n",
    "year_str=dbutils.widgets.get(\"year\")\n",
    "month_str=dbutils.widgets.get(\"month\")\n",
    "print(year_str,month_str)\n",
    "# Convert year and month to integers\n",
    "year_int = int(year_str) if year_str else None\n",
    "month_int = int(month_str) if month_str else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6ddbf83-9cb3-4b50-8026-7df34895ce38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rename_and_cleanup_csv(path: str, filename: str):\n",
    "    files = dbutils.fs.ls(path)\n",
    "    part_file = None\n",
    "    for f in files:\n",
    "        if f.name.startswith('part-') and f.name.endswith('.csv'):\n",
    "            part_file = f.name\n",
    "            break\n",
    "    if not part_file:\n",
    "        raise FileNotFoundError(f'No part file found in {path}')\n",
    "    source = f'{path}/{part_file}'\n",
    "    dest = f'{path}/{filename}'\n",
    "    dbutils.fs.mv(source, dest)\n",
    "    print(f'Renamed {source} to {dest}')\n",
    "    # Remove all other files except the CSV\n",
    "    files = dbutils.fs.ls(path)\n",
    "    for f in files:\n",
    "        if f.name != filename:\n",
    "            dbutils.fs.rm(f'{path}/{f.name}')\n",
    "            print(f'Removed {path}/{f.name}')\n",
    "\n",
    "# Example usage:\n",
    "# rename_and_cleanup_csv('/Volumes/retail_analytics/portfolio/amfi_data/stock_returns.csv', 'stock_returns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c839693-8888-424c-82bb-db17e13dc2b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('/Volumes/retail_analytics/portfolio/amfi_data/AMFI data.csv', header=True, sep=';',inferSchema=True)\n",
    "df.createOrReplaceTempView('amfi_data')\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ab517db-4b3c-4345-9df2-7262fb9ed839",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "portfolio_df=spark.read.csv('/Volumes/retail_analytics/portfolio/amfi_data/Portfolio_data_feb0226.csv',header=True,inferSchema=True)\n",
    "portfolio_df.createOrReplaceTempView('portfolio_data')\n",
    "# display(portfolio_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58ad48a9-ca79-43d3-a788-65f4fa1f83d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, round, regexp_replace, year, month, to_date\n",
    "\n",
    "# Portfolio data\n",
    "df_jan_2026 = spark.read.csv(\n",
    "    '/Volumes/retail_analytics/portfolio/amfi_data/Portfollio_holdings_202601.csv',\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df_jan_2026 = df_jan_2026.withColumn(\n",
    "    'invested_amount',\n",
    "    round(col('Quantity Available') * col('Average Price'), 2)\n",
    ").withColumn(\n",
    "    'total_value',\n",
    "    round(col('invested_amount') + col('Unrealized P&L'), 2)\n",
    ")\n",
    "\n",
    "df_jan_2026.createOrReplaceTempView('portfolio_zerodha')\n",
    "\n",
    "\n",
    "# Stock price history\n",
    "stock_price_history = spark.read.csv(\n",
    "    '/Volumes/retail_analytics/portfolio/amfi_data/all_stocks_monthly_returns.csv',\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "stock_price_history = stock_price_history.withColumn(\n",
    "    'ticker',\n",
    "    regexp_replace(col('ticker'), '\\\\.NS$', '')\n",
    ").withColumn(\n",
    "    'YearMonth_date',\n",
    "    to_date(col('YearMonth'), 'yyyy-MM-dd')   # adjust format if needed\n",
    ").withColumn(\n",
    "    'year',\n",
    "    year(col('YearMonth_date'))\n",
    ").withColumn(\n",
    "    'month',\n",
    "    month(col('YearMonth_date'))\n",
    ")\n",
    "\n",
    "stock_price_history.createOrReplaceTempView('stock_price_history')\n",
    "\n",
    "\n",
    "# Mutual fund price history\n",
    "mf_price_history = spark.read.csv(\n",
    "    '/Volumes/retail_analytics/portfolio/amfi_data/mf_monthly_returns.csv',\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "mf_price_history = mf_price_history.withColumn(\n",
    "    'YearMonth_date',\n",
    "    to_date(col('YearMonth'), 'yyyy-MM-dd')   # adjust format if needed\n",
    ").withColumn(\n",
    "    'year',\n",
    "    year(col('YearMonth_date'))\n",
    ").withColumn(\n",
    "    'month',\n",
    "    month(col('YearMonth_date'))\n",
    ")\n",
    "\n",
    "mf_price_history.createOrReplaceTempView('mf_price_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd10624-dfe9-47d3-b405-6d40fd2a060d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Portfolio and stock price history setup"
    }
   },
   "outputs": [],
   "source": [
    "# df_jan_2026=spark.read.csv('/Volumes/retail_analytics/portfolio/amfi_data/Portfollio_holdings_202601.csv', header=True,inferSchema=True)\n",
    "# df_jan_2026=df_jan_2026.withColumn('invested_amount', round(col('Quantity Available')*col('Average Price'), 2))\\\n",
    "#     .withColumn('total_value', round(col('invested_amount')+col('Unrealized P&L'), 2))\n",
    "\n",
    "# df_jan_2026.createOrReplaceTempView('portfolio_zerodha')\n",
    "# stock_price_history=spark.read.csv('/Volumes/retail_analytics/portfolio/amfi_data/all_stocks_monthly_returns.csv', header=True,inferSchema=True)\n",
    "# stock_price_history = stock_price_history.withColumn(\n",
    "#     'ticker', regexp_replace('ticker', '\\\\.NS$', '')\n",
    "# )\\\n",
    "#     .withColumn('year', year('YearMonth'))\\\n",
    "#     .withColumn('month', month('YearMonth'))\n",
    "\n",
    "# stock_price_history.createOrReplaceTempView('stock_price_history')\n",
    "# mf_price_history=spark.read.csv('/Volumes/retail_analytics/portfolio/amfi_data/mf_monthly_returns.csv', header=True,inferSchema=True)\n",
    "# mf_price_history=mf_price_history.withColumn('year', year('YearMonth'))\\\n",
    "#     .withColumn('month', month('YearMonth'))\n",
    "# mf_price_history.createOrReplaceTempView('mf_price_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6818b23-20f1-454e-b2f2-d457136f77e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "individual stock returns"
    }
   },
   "outputs": [],
   "source": [
    "stock_returns=spark.sql(f\"\"\"select symbol,sector,pft.year,pft.month,invested_amount,total_value,round(total_value * monthly_return_pct / 100,2) AS profit_loss,round(total_value+profit_loss,2) as final_value from stock_price_history st join portfolio_zerodha pft on st.ticker=pft.symbol and st.year=pft.year and st.month=pft.month where pft.year={year_int} and pft.month={month_int} \"\"\")\n",
    "\n",
    "stock_returns.write.mode('overwrite').partitionBy('year','month','symbol').saveAsTable('retail_analytics.portfolio.stock_returns')\n",
    "\n",
    "stock_returns.createOrReplaceTempView('stock_returns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd31be81-8bef-4a43-93e7-4ffd8a9b1ed6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "individual MF returns"
    }
   },
   "outputs": [],
   "source": [
    "#### MF RETURNS #####\n",
    "mf_returns=spark.sql(f\"\"\"with cte as (\n",
    "select scheme_code,* from portfolio_zerodha pft join amfi_data amfi on pft.isin=amfi.isin where pft.year={year_int} and pft.month={month_int} and symbol!='GOLDBEES')\n",
    "select c.Symbol,c.invested_amount,c.total_value,round(c.total_value * monthly_return_pct / 100,2) AS profit_loss,monthly_return_pct,c.Year,c.Month from cte c join mf_price_history mf on c.scheme_code=mf.scheme_code and mf.year=c.year and mf.month=c.month\n",
    "                     \"\"\")\n",
    "\n",
    "mf_returns.write.mode('overwrite').partitionBy('year','month','Symbol').saveAsTable('retail_analytics.portfolio.mf_returns')\n",
    "mf_returns.createOrReplaceTempView('mf_returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "699713bc-c79f-465b-a789-cbcb881b45cb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stock portfolio"
    }
   },
   "outputs": [],
   "source": [
    "stock_portfolio=spark.sql(\"\"\"with cte as (\n",
    "select 'STOCKS' as portfolio_type,round(sum(invested_amount),2) as invested_amount,round(sum(total_value),2) as mnt_stock_portfolio_value,round(sum(total_value+profit_loss),2) as month_end_value,month,year from stock_returns\n",
    "  group by month,year)\n",
    "\n",
    "select portfolio_type,invested_amount,mnt_stock_portfolio_value,month_end_value,round(month_end_value-mnt_stock_portfolio_value,2) as month_profit_loss,round(month_profit_loss/month_end_value*100,2)as loss_profit_pct_month, month,year from cte\"\"\")\n",
    "stock_portfolio.createOrReplaceTempView('stock_portfolio')\n",
    "stock_portfolio.write.mode('overwrite').partitionBy('year','month','portfolio_type').saveAsTable('retail_analytics.portfolio.stock_portfolio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "156c0bfa-2c9d-407a-a3c0-5d0c94d4d9d5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "MF portfolio"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "mf_portfolio=spark.sql(\"\"\"with cte as (select 'MF' as portfolio_type,round(sum(invested_amount),2) as invested_amount,round(sum(total_value),2) as mnt_stock_portfolio_value,round(sum(total_value+profit_loss),2) as month_end_value,month,year from mf_returns\n",
    "  group by month,year)\n",
    "\n",
    "select portfolio_type,invested_amount,mnt_stock_portfolio_value,month_end_value,round(month_end_value-mnt_stock_portfolio_value,2) as month_profit_loss,round(month_profit_loss/month_end_value*100,2)as loss_profit_pct_month, month,year from cte\"\"\")\n",
    "mf_portfolio.createOrReplaceTempView('mf_portfolio')\n",
    "mf_portfolio.write.mode(\"overwrite\").partitionBy('year','month','portfolio_type').saveAsTable('retail_analytics.portfolio.mf_portfolio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a14d683b-cb24-443d-be6a-dfd5d9c4d84f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "consolidated Portfolio"
    }
   },
   "outputs": [],
   "source": [
    "consolidated_df=spark.sql(\"\"\" with portfolio_returns as (\n",
    "select * from mf_portfolio\n",
    "union \n",
    "select * from stock_portfolio)\n",
    "select 'MF+STOCK' as portfolio_type,round(sum(invested_amount),2) as invested_amount,round(sum(mnt_stock_portfolio_value),2) as mnt_stock_portfolio_value,round(sum(month_end_value),2) as month_end_value,round(sum(month_profit_loss),2) as month_profit_loss,round(sum(loss_profit_pct_month),2) as loss_profit_pct_month,month,year from portfolio_returns\n",
    "group by month,year\n",
    "order by year,month\n",
    "                          \"\"\")\n",
    "consolidated_df.write.mode('overwrite').partitionBy('year','month','portfolio_type').saveAsTable('retail_analytics.portfolio.consolidated_portfolio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39a76160-a2e1-4b89-978c-f6f47a2ee933",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771693805839}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "testing"
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# with cte as (\n",
    "# select 'STOCKS' as portfolio_type,round(sum(invested_amount),2) as invested_amount,round(sum(total_value),2) as mnt_stock_portfolio_value,round(sum(total_value+profit_loss),2) as month_end_value,month,year from stock_returns\n",
    "#   group by month,year),\n",
    "# stc_returns as (\n",
    "# select portfolio_type,invested_amount,mnt_stock_portfolio_value,month_end_value,round(month_end_value-mnt_stock_portfolio_value,2) as month_profit_loss,round(month_profit_loss/month_end_value*100,2)as loss_profit_pct_month, month,year from cte),\n",
    "# cte2 as (\n",
    "# select 'MF' as portfolio_type,round(sum(invested_amount),2) as invested_amount,round(sum(total_value),2) as mnt_stock_portfolio_value,round(sum(total_value+profit_loss),2) as month_end_value,month,year from mf_returns\n",
    "#   group by month,year),\n",
    "# mfc_returns as (\n",
    "# select portfolio_type,invested_amount,mnt_stock_portfolio_value,month_end_value,round(month_end_value-mnt_stock_portfolio_value,2) as month_profit_loss,round(month_profit_loss/month_end_value*100,2)as loss_profit_pct_month, month,year from cte2),\n",
    "# portfolio_returns as (\n",
    "# select * from mfc_returns\n",
    "# union \n",
    "# select * from stc_returns)\n",
    "# select 'MF+STOCK' as portfolio_type,round(sum(invested_amount),2) as invested_amount,round(sum(mnt_stock_portfolio_value),2) as mnt_stock_portfolio_value,round(sum(month_end_value),2) as month_end_value,round(sum(month_profit_loss),2) as month_profit_loss,round(sum(loss_profit_pct_month),2) as loss_profit_pct_month,month,year from portfolio_returns\n",
    "# group by month,year\n",
    "# order by year,month\n",
    "# --select * from portfolio_returns where portfolio_type='MF+STOCK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac4664f4-9c8c-4048-b8eb-b66d5663a526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls('/Workspace/Users/umesh.ravi0595@outlook.com/MfPortfolioGenai/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22caf58d-d43d-47c8-8b7f-01fb6ebfe211",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Export as csv files"
    }
   },
   "outputs": [],
   "source": [
    "stock_returns = spark.table('retail_analytics.portfolio.stock_returns')\n",
    "stock_returns.coalesce(1).write.mode('overwrite').option('header', True).csv('/Volumes/retail_analytics/portfolio/amfi_data/stock_returns.csv')\n",
    "\n",
    "\n",
    "mf_returns = spark.table('retail_analytics.portfolio.mf_returns')\n",
    "mf_returns.coalesce(1).write.mode('overwrite').option('header', True).csv('/Volumes/retail_analytics/portfolio/amfi_data/mf_returns.csv')\n",
    "\n",
    "stock_portfolio = spark.table('retail_analytics.portfolio.stock_portfolio')\n",
    "stock_portfolio.coalesce(1).write.mode('overwrite').option('header', True).csv('/Volumes/retail_analytics/portfolio/amfi_data/stock_portfolio.csv')\n",
    "\n",
    "mf_portfolio = spark.table('retail_analytics.portfolio.mf_portfolio')\n",
    "mf_portfolio.coalesce(1).write.mode('overwrite').option('header', True).csv('/Volumes/retail_analytics/portfolio/amfi_data/mf_portfolio.csv')\n",
    "\n",
    "consolidated_portfolio = spark.table('retail_analytics.portfolio.consolidated_portfolio')\n",
    "consolidated_portfolio.coalesce(1).write.mode('overwrite').option('header', True).csv('/Volumes/retail_analytics/portfolio/amfi_data/consolidated_portfolio.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f7c822-9e4d-457d-bd2f-cdf22019bc87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path=['/Volumes/retail_analytics/portfolio/amfi_data/stock_returns.csv','/Volumes/retail_analytics/portfolio/amfi_data/mf_returns.csv','/Volumes/retail_analytics/portfolio/amfi_data/stock_portfolio.csv','/Volumes/retail_analytics/portfolio/amfi_data/mf_portfolio.csv','/Volumes/retail_analytics/portfolio/amfi_data/consolidated_portfolio.csv']\n",
    "filename=['stock_returns.csv','mf_returns.csv','stock_portfolio.csv','mf_portfolio.csv','consolidated_portfolio.csv']\n",
    "for i, n in zip(path, filename):\n",
    "    print(i, n)\n",
    "    rename_and_cleanup_csv(i, n)\n",
    "# dbutils.fs.rm('/Volumes/retail_analytics/portfolio/amfi_data/stock_returns.csv')\n",
    "# dbutils.fs.rm('/Volumes/retail_analytics/portfolio/amfi_data/mf_returns.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e8a4c73-dcf6-44de-9223-72d8ec586ad4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Rename and cleanup CSV export directory"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7567291197571109,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "portfolio returns",
   "widgets": {
    "month": {
     "currentValue": "1",
     "nuid": "e346d08b-165f-4ee8-8c74-d26162df30bb",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "month",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "month",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "year": {
     "currentValue": "2026",
     "nuid": "b0b4d3b7-657e-4c01-a45d-54e21fd4a5bb",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "year",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "year",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
